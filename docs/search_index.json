[
["index.html", "Efficiency of Univariate Kernel Density Estimation with TensorFlow Bachelor Thesis Abstract", " Efficiency of Univariate Kernel Density Estimation with TensorFlow Bachelor Thesis Marc Steiner Abstract An implementation of a one-dimensional Kernel Density Estimation in TensorFlow/Zfit is proposed. Starting from the basic algorithm, several optimizations from recent papers are introduced and combined to ameliorate the efficiency of the algorithm. By comparing its accuracy and efficiency to implementations in pure Python it is shown as competitive and useful in real world applications. "],
["introduction.html", "1 Introduction 1.1 Kernel Density Estimation 1.2 zfit and TensorFlow", " 1 Introduction 1.1 Kernel Density Estimation In many fields of science and in physics especially, scientists need to estimate the probability density function (PDF) from which a set of data is drawn without having a approximate model of the underlying mechanisms, since they are too complex to be fully understood analytically. So called parametric methods fail, because the knowledge of the system is to poor to design a model, which parameters can then be fitted by some goodness-of-fit criterion like log-likelihood or \\(\\chi^2\\). In the particle accelerator at CERN for instance, they record a whooping 25 gigabytes of data per second1, resulting from the process of many physical interactions that occur almost simultaneously, making it impossible to anticipate features of the distribution one observes. To combat this so called non-parametric methods like histograms are used. By summing the the data up in discrete bins we can approximate the underlying parent distribution, without needing any knowledge of the physical interactions. However there are also many more sophisticated non-parametric methods, one in particular is Kernel Density Estimation (KDE), which can be looked at as a sort of generalized histogram.2 Histograms tend to produce PDFs that are highly dependent on bin width and bin positioning, meaning the interpretion of the data changes by a lot by two arbitrary parameters. KDE circumvents this problem by replacing each data point with a so called kernel that specifies how much it influences its neighbouring regions as well. The kernels themselves are centered at the data point directly, eliminating the need for arbitrary bin positioning3. Since KDE still depends on kernel width (instead of bin width), one might argue that this is not a major improvement. However, upon closer inspection, one finds that the underlying PDF does depend less strongly on the kernel width than histograms on bin width and it is much easier to specify rules for an approximately optimal kernel width than it is to do so for bin width4. In addition, by specifying a smooth kernel, one gets a smooth distribution as well, which is often desirable or even expected from theory. Due to this increased robustness, KDE is particular useful in High-Energy Physics (HEP) where it has been used for confidence level calculations for the Higgs Searches at the Large Electron Positron Collider (LEP)5. However there is still room for improvement and certain more sophisticated approaches to Kernel Density Estimation have been proposed in dependence on specific areas of application5. 1.2 zfit and TensorFlow Currently the basic principle of KDE has been implemented in various programming languages and statistical modeling tools. The standard framework used in HEP, that includes KDE is the ROOT/RooFit toolkit written in C++. However, Python plays an increasingly large role in the natural sciences due to support by corporations involved in Big Data and its superior accessibility. To elevate research in HEP, zfit, a new alternative to RooFit, was proposed. It is implemented on top of TensorFlow, one of the leading Python frameworks to handle large data and high parallelization, allowing a transparent usage of CPUs and GPUs6. So far there exists no direct implementation of Kernel Density Estimation in zfit nor TensorFlow, but various implementations in Python. This implementations will be discussed in the next chapter (2) before I propose an implementation of Kernel Density Estimation in TensorFlow (3). Starting with a rather simple implementation, multiple improvements from recent papers are integrated to ameliorate its efficiency. Efficiency and accuracy of the implementation are then tested by comparing it to other implementations in pure Python and simple smoothed histograms (4). In the last chapter (5) I compare its accurracy and efficiency to smoothed histograms and implementations in pure Python. References "],
["currentState.html", "2 Current state of the art", " 2 Current state of the art … For humongous data streams (like at CERN 1), Kernel Density Estimation itself needs to be approximated. … References "],
["tensorflowImplementation.html", "3 Implementation 3.1 Exact Kernel Density Estimation 3.2 Simple and linear Binning 3.3 Using convolution and the Fast Fourier Transform 3.4 Improved Sheather Jones Algorithm", " 3 Implementation 3.1 Exact Kernel Density Estimation The implementation of a simple Kernel Density Estimation in TensorFlow is straightforward. As described in the original Tensorflow Probability Paper7, a KDE can be constructed by using its MixtureSameFamily Distribution, given sampled data as follows from tensorflow_probability import distributions as tfd f = lambda x: tfd.Independent(tfd.Normal(loc=x, scale=1.)) n = data.shape[0].value kde = tfd.MixtureSameFamily( mixture_distribution=tfd.Categorical( probs=[1 / n] * n), components_distribution=f(data)) Interestingly, due to the smartly capsulated structure of TensorFlow Probability we can use any distribution of the loc-scale family type as a Kernel, if there exists an implementation for it in TensorFlow Probability. If the used Kernel has bounded support, the implementation proposed in this paper allows to specify the support upon instanciation of the class. If the Kernel has infinite support (like a Gaussian kernel i. e.) a practical support estimate is calculated by searching for approximative roots with the Brent’s method8 implemented in TensorFlow the python package tf_quant_finance by Google. This allows us to speed up the calculation However calculating an exact Kernel Density Estimation is not always feasible as this can take a long time with a huge collection of events, especially in high energy physics. By implementing it in TensorFlow we already get a significant speed up compared to implementations in native Python, since most of TensorFlow is actually implemented in C++ and the code is optimized before running. The computational complexity however, remains the same nonetheless. \\[\\begin{equation} \\widehat{f}_h(x) = \\frac{1}{nh} \\sum_{k=1}^n K\\Big(\\frac{x-x_k}{h}\\Big) \\tag{3.1} \\end{equation}\\] The computational complexity of the basic exact KDE above is \\(\\mathcal{O}(nm)\\) where \\(n\\) is the number of sample points to estimate from and \\(m\\) is the number of evaluation points (the points where you want to calculate the estimate). To combat this complexity several methods exist. 3.2 Simple and linear Binning The most straightforward way to decrease runtime is by limiting the number of sample points. This can be done by a binning routine, where the values at a smaller number of regular grid points are estimated from the original large number of sample points. Given a set of sample points \\(X = \\{x_0, x_1, ..., x_k, ..., x_{n-1}, x_n\\}\\) with weights \\(w_k\\) and a set of equally spaced grid points \\(G = \\{g_0, g_1, ..., g_l, ..., g_{n-1}, g_M\\}\\) where \\(N &lt; n\\) we can assign an estimate (or a count) \\(c_l\\) to each grid point \\(g_l\\) and use the newly found \\(g_l\\)’s to calculate the kernel density estimation instead. This brings the computational complexity down to \\(\\mathcal{O}(Nm)\\). Depending on the number of grid points \\(N\\) the estimate is either more accurate and slower or less accurate and faster. However as we will see in the comparison chapter later as well, even a grid of size \\(1024\\) is enough to capture the true density with high accuracy on a million data points.9. As described in the excellent overview by Artur Gramacki10 simple binning or linear binning can be used, although the last is often preferred since it is more accurate and the difference in computational complexity is negligible. Simple binning is just the standard process of taking a weighted histogram that is divided by the sum of the sample points weights (normalization). In one dimension simple binning is binary in that it assigns a data points weight (\\(1\\) for an unweighted historgram) either to the bin left or right of itself. Linear binning on the other hand assigns a fraction of the whole weight to both bins on either side, proportional to the closeness of bin and data point in relation to the bin width. Mathematically linear binning in one dimension can be calculated like this: \\[\\begin{equation} c_l = c(g_l) = \\sum_{\\substack{x_k \\in X\\\\g_l &lt; x_k &lt; g_{l+1}}} \\frac{g_{k+1}-x_k}{g_{l+1} - g_l} \\cdot w_k + \\sum_{\\substack{x_k \\in X\\\\g_{l-1} &lt; x_k &lt; g_l}} \\frac{x_k - g_{l-1}}{g_{l+1} - g_l} \\cdot w_k \\tag{3.2} \\end{equation}\\] Implementing linear binning efficiently with TensorFlow is a bit tricky since loops should be avoided. However with some inspiration from the excellent KDEpy package11 which implements a kernel density estimation in native python/cython this can be done without using loops at all. By transforming the data such that every data point \\(x_k\\) can be described by an integral part (corresponding to its nearest left grid point number \\(l\\)) plus some fractional part (corresponding to the distance between grid point \\(g_l\\) and data point \\(x_k\\)) and applying tf.math.bincount twice to the transformed integral part weighting it with the fractional part times the initial weight. The kernel density estimation can then be calculated as a mixture distribution of kernels located at the grid points, weighted with their associated grid count. \\[\\begin{equation} \\widehat{f}_h(x) = \\frac{1}{nh} \\sum_{l=1}^N c_l \\cdot K\\Big(\\frac{x-g_l}{h}\\Big) \\tag{3.3} \\end{equation}\\] 3.3 Using convolution and the Fast Fourier Transform With binning implemented, another technique to speed up the computation is rewriting the Kernel Density Estimation as convolution operation between the kernel and the grid counts calculated by the binning routine. By using the fact that a convolution is just a multiplication in Fourier space one can reduce the computational complexity down to \\(\\mathcal{O}(\\log{N} \\cdot m)\\).10 Using the equation (3.3) from above but also only evaluating it at grid points gives us \\[\\begin{equation} \\widehat{f}_h(g_j) = \\frac{1}{nh} \\sum_{l=1}^N c_l \\cdot K\\Big(\\frac{g_j-g_l}{h}\\Big) = \\frac{1}{nh} \\sum_{l=1}^N k_{j-l} \\cdot c_l \\tag{3.4} \\end{equation}\\] where \\(k_{j-l} = K(\\frac{g_j-g_l}{h})\\). If we set \\(c_l = 0\\) for all \\(l\\) not in the set \\(\\{1, ..., N\\}\\) and notice that \\(K(-x) = K(x)\\) we can extend equation (3.4) to a discrete convolution as follows \\[\\begin{equation} \\widehat{f}_h(g_j) = \\frac{1}{nh} \\sum_{l=-N}^N k_{j-l} \\cdot c_l = \\vec{c} \\ast \\vec{k} \\tag{3.5} \\end{equation}\\] where the two vectors look like this knitr::include_graphics(&#39;figures/c_conv_k.png&#39;) Figure 3.1: Vectors \\(\\vec{c}\\) and \\(\\vec{k}\\) By using the well known convolution theorem we can fourier transform \\(\\vec{c}\\) and \\(\\vec{k}\\) multiply them and inverse fourier transform them back into real space. In TensorFlow convolutions are efficiently implemented already in this way if we use tf.nn.conv1d. In benchmarking using this method proved significantly faster than using tf.signal.rfft and tf.signal.irfft to transform, multiply and inverse transform the vectors, which is implemented as an alternative as well. This algorithm is implemented as its own class since it does not represent a complete mixture distribution anymore but calculates just the density distribution values at the speficied grid points. To still infer values for other points in the range of \\(x\\) tfp.math.interp_regular_1d_grid is used which computes a linear interpolation of values between the grid. 3.4 Improved Sheather Jones Algorithm A different take on Kernel Density Estimators is described in the paper ‘Kernel density estimation by diffusion’ by Botev et al.12. The authors present a new adaptive kernel density estimator based on linear diffusion processes which also includes an estimation for the optimal bandwidth. The algorithm is quite difficult to understand, a detailed explanation is given in the ‘Handbook of Monte Carlo Methods’13 by the original paper authors. However the general idea is briefly sketched below. A critical insight is that the Gaussian kernel density estimator \\(\\widehat{f}_{h,norm}\\) is the solution of the partial differential equation \\[\\begin{equation} \\frac{\\partial}{\\partial t} \\widehat{f}_{h,norm}(x,t) = \\frac{1} {2} \\frac{\\partial^2}{\\partial x^2} \\widehat{f}_{h,norm}(x,t),\\space t&gt;0 \\tag{3.6} \\end{equation}\\] with \\(x \\in \\mathbb{R}\\), \\(\\lim_{x\\rightarrow \\pm \\infty}{\\widehat{f}_{h,norm}(x,t) = 0}\\) and initial condition \\(\\widehat{f}_{h,norm}(x,0) = \\Delta(x)\\), where \\(\\Delta(x) = \\frac{1}{N} \\sum_{k=0}^N \\delta_{x_k}(x)\\) us the empirical density of the given sample points \\(X = \\{x_0, x_1, ..., x_k, ..., x_{n-1}, x_n\\}\\) and \\(\\delta_{x_k}(x)\\) is the Dirac measure at \\(x_k\\). This means the kernel density estimator can be obtained by evolving the solution of the partial differential equation (3.6) up to time \\(t\\). The key observation is that (3.6) can be solved on a finite domain efficiently using the fast cosine transform - an FFT-related transform.13 The optimal bandwidth is often defined as the one that minimizes the mean integrated square error(\\(MISE\\)) \\[\\begin{equation} MISE(t) = \\mathbb{E}_f\\int [\\widehat{f}_{h,norm}(x,t) - f(x)]^2 dx \\tag{3.7} \\end{equation}\\] An asymptotically optimal value \\(t^{\\ast}\\) which minimizes a first-order asymptotic appoximation of the \\(MISE\\) is then given by13 \\[\\begin{equation} t^{\\ast} = \\Big( \\frac{1}{2N\\sqrt{\\pi} \\| f&#39;&#39;\\|^2}\\Big)^{\\frac{2}{5}} \\tag{3.8} \\end{equation}\\] Using the fact that \\(\\|f^{(j)}\\|^2 = (-1)^j \\mathbb{E}_f[f^{(2j)}(X)], \\space j\\geq 1\\) and an initial estimation for \\(\\|\\widehat{f}_{h,norm}^{(l+2)}\\|^2\\) for some \\(l \\geq 3\\) one can then iteratively get a an estimation for \\(\\|\\widehat{f}_{h,norm}^{(2)}\\|^2\\) which can then be used to estimate \\(t^{\\ast}\\) instead of \\(\\|f&#39;&#39;\\|^2\\). According to their handbook \\(l = 7\\) is a suitable value to yield good practical results. The improvment compared to the standard Sheather-Jones plug-on method14 consists in the fact that to compute the initial estimation of \\(\\|\\widehat{f}_{h,norm}^{(l+2)}\\|^2\\) is calculated by solving the partial differental equation using the fast cosine transform as described above, eliminating the need to assume normally distributed data for the initial estimate and leading improved performance, especially for density distributions that are far from normal as seen in the next chapter. The implementation of the algorithm in TensorFlow proposed in this paper was also inspired a lot by the python package KDEpy11 and uses Brent’s method8 to find roots implemented in TensorFlow the python package tf_quant_finance as well. One shortcoming of the improved Sheather-Jones algorithm (ISJ) is that with few data points to estimate from, it is not guarenteed to converge. If that happens, one has to use the exact, binned or FFT kernel density estimators as described above. References "],
["comparison.html", "4 Comparison 4.1 Benchmark setup 4.2 Basic implementation against Binned and FFT implementations 4.3 New implementation against KDEpy 4.4 New implementation run with GPU support", " 4 Comparison To show the efficiency and performance of the Kernel Density Estimation methods implemented with TensorFlow a benchmarking suite was developed. It consists of three parts, a collection of distributions, a collection of methods to compare and a runner module that implements helper methods to execute the methods to test against the different distributions and plot the generated dataset nicely. 4.1 Benchmark setup To compare the different implementations multiple popular test distributions mentioned in Wand et al.15 were used. A simple normal distribution, a simple uniform distribution, a bimodal distribution comprised of two normals, a skewed bimodal distribution, a claw distribution that has spikes and one called asymmetric double claw that has different sized spikes left and right. All comparisons were made using a standard Gaussian Kernel. Although all loc-scale family distributions of TensorFlow Probability may be used for the new implmentation proposed in this paper may be used, the Gaussian kernel is the most used one and provides best reference to compare different implementations against eachother. 4.2 Basic implementation against Binned and FFT implementations 4.2.1 Runtime 4.2.2 Accuracy 4.3 New implementation against KDEpy 4.3.1 Runtime 4.3.2 Accuracy 4.4 New implementation run with GPU support 4.4.1 Runtime 4.4.2 Accuracy References "],
["summary.html", "5 Summary", " 5 Summary In summary we can conclude that… "],
["references.html", "References", " References "]
]
