[
["index.html", "Efficiency of Univariate Kernel Density Estimation with TensorFlow Bachelor Thesis Abstract", " Efficiency of Univariate Kernel Density Estimation with TensorFlow Bachelor Thesis Marc Steiner Abstract An implementation of a one-dimensional Kernel Density Estimation in TensorFlow is proposed. Starting from the basic algorithm, several optimizations from recent papers are introduced and combined to ameliorate the efficiency of the algorithm. By comparing its accuracy and efficiency to implementations in pure Python as well to density estimations by smoothed histograms it is shown as competitive and useful in real world applications. "],
["introduction.html", "1 Introduction 1.1 Kernel Density Estimation 1.2 zfit and TensorFlow", " 1 Introduction 1.1 Kernel Density Estimation In many fields of science and in physics especially, scientists need to estimate the probability density function (PDF) from which a set of data is drawn without having a approximate model of the underlying mechanisms, since they are too complex to be fully understood analytically. So called parametric methods fail, because the knowledge of the system is to poor to design a model, which parameters can then be fitted by some goodness-of-fit criterion like log-likelihood or \\(\\chi^2\\). In the particle accelerator at CERN for instance, they record a whooping 25 gigabytes of data per second1, resulting from the process of many physical interactions that occur almost simultaneously, making it impossible to anticipate features of the distribution one observes. To combat this so called non-parametric methods like histograms are used. By summing the the data up in discrete bins we can approximate the underlying parent distribution, without needing any knowledge of the physical interactions. However there are also many more sophisticated non-parametric methods, one in particular is Kernel Density Estimation (KDE), which can be looked at as a sort of generalized histogram.2 Histograms tend to produce PDFs that are highly dependent on bin width and bin positioning, meaning the interpretion of the data changes by a lot by two arbitrary parameters. KDE circumvents this problem by replacing each data point with a so called kernel that specifies how much it influences its neighbouring regions as well. The kernels themselves are centered at the data point directly, eliminating the need for arbitrary bin positioning3. Since KDE still depends on kernel width (instead of bin width), one might argue that this is not a major improvement. However, upon closer inspection, one finds that the underlying PDF does depend less strongly on the kernel width than histograms on bin width and it is much easier to specify rules for an approximately optimal kernel width than it is to do so for bin width4. In addition, by specifying a smooth kernel, one gets a smooth distribution as well, which is often desirable or even expected from theory. Due to this increased robustness, KDE is particular useful in High-Energy Physics (HEP) where it has been used for confidence level calculations for the Higgs Searches at the Large Electron Positron Collider (LEP)5. However there is still room for improvement and certain more sophisticated approaches to Kernel Density Estimation have been proposed in dependence on specific areas of application5. 1.2 zfit and TensorFlow Currently the basic principle of KDE has been implemented in various programming languages and statistical modeling tools. The standard framework used in HEP, that includes KDE is the ROOT/RooFit toolkit written in C++. However, Python plays an increasingly large role in the natural sciences due to support by corporations involved in Big Data and its superior accessibility. To elevate research in HEP, zfit, a new alternative to RooFit, was proposed. It is implemented on top of TensorFlow, one of the leading Python frameworks to handle large data and high parallelization, allowing a transparent usage of CPUs and GPUs6. So far there exists no direct implementation of Kernel Density Estimation in zfit nor TensorFlow, but various implementations in Python. This implementations will be discussed in the next chapter (2) before I propose an implementation of Kernel Density Estimation in TensorFlow (3). Starting with a rather simple implementation, multiple improvements from recent papers are integrated to ameliorate its efficiency. Efficiency and accuracy of the implementation are then tested by comparing it to other implementations in pure Python and simple smoothed histograms (4). In the last chapter (5) I compare its accurracy and efficiency to smoothed histograms and implementations in pure Python. References "],
["currentState.html", "2 Current state of the art", " 2 Current state of the art … For humongous data streams (like at CERN 1), Kernel Density Estimation itself needs to be approximated. … References "],
["tensorflowImplementation.html", "3 Implementation", " 3 Implementation The following is an implementation of Kernel Density Estimation using TensorFlow. import numpy as np import tensorflow as tf from tensorflow_probability.python.distributions import Categorical from tensorflow_probability.python.distributions import Independent from tensorflow_probability.python.distributions import MixtureSameFamily from tensorflow_probability.python.distributions import Normal from tensorflow_probability.python.distributions import distribution from tensorflow_probability.python.distributions.normal import Normal from tensorflow_probability.python.internal import assert_util from tensorflow_probability.python.internal import dtype_util from tensorflow_probability.python.internal import reparameterization from tensorflow_probability.python.internal import tensor_util class KernelDensityEstimation(MixtureSameFamily): &quot;&quot;&quot; Kernel density estimation based on data. Implements Linear Binning and Fast Fourier Transform to speed up the computation. &quot;&quot;&quot; def __init__(self, data, bandwidth=0.01, kernel=Normal, use_grid=False, use_fft=False, num_grid_points=1024, reparameterize=False, validate_args=False, allow_nan_stats=True, name=&#39;KernelDensityEstimation&#39;): components_distribution_generator = lambda loc, scale: Independent(kernel(loc=loc, scale=scale)) with tf.name_scope(name) as name: self._use_fft=use_fft dtype = dtype_util.common_dtype([bandwidth, data], tf.float32) self._bandwidth = tensor_util.convert_nonref_to_tensor( bandwidth, name=&#39;bandwidth&#39;, dtype=dtype) self._data = tensor_util.convert_nonref_to_tensor( data, name=&#39;data&#39;, dtype=dtype) if(use_fft): self._grid = self._generate_grid(num_grid_points) self._grid_data = self._linear_binning() mixture_distribution=Categorical(probs=self._grid_data) components_distribution=components_distribution_generator(loc=self._grid, scale=self._bandwidth) elif(use_grid): self._grid = self._generate_grid(num_grid_points) self._grid_data = self._linear_binning() mixture_distribution=Categorical(probs=self._grid_data) components_distribution=components_distribution_generator(loc=self._grid, scale=self._bandwidth) else: self._grid = None self._grid_data = None n = self._data.shape[0] mixture_distribution=Categorical(probs=[1 / n] * n) components_distribution=components_distribution_generator(loc=self._data, scale=self._bandwidth) super(KernelDensityEstimation, self).__init__( mixture_distribution=mixture_distribution, components_distribution=components_distribution, reparameterize=reparameterize, validate_args=validate_args, allow_nan_stats=allow_nan_stats, name=name ) def _generate_grid(self, num_points): minimum = tf.math.reduce_min(self._data) maximum = tf.math.reduce_max(self._data) return tf.linspace(minimum, maximum, num=num_points) def _linear_binning(self, weights=None): if weights is None: weights = np.ones_like(self._data) grid_min = tf.math.reduce_min(self._grid) grid_max = tf.math.reduce_max(self._grid) num_intervals = tf.math.subtract(tf.size(self._grid), tf.constant(1)) dx = tf.math.divide(tf.math.subtract(grid_max, grid_min), tf.cast(num_intervals, tf.float32)) transformed_data = tf.math.divide(tf.math.subtract(self._data, grid_min), dx) # Compute the integral and fractional part of the data # The integral part is used for lookups, the fractional part is used # to weight the data integral = tf.math.floor(transformed_data) fractional = tf.math.subtract(transformed_data, integral) # Compute the weights for left and right side of the linear binning routine frac_weights = tf.math.multiply(fractional, weights) neg_frac_weights = tf.math.subtract(weights, frac_weights) # If the data is not a subset of the grid, the integral values will be # outside of the grid. To solve the problem, we filter these values away #unique_integrals = np.unique(integral) #unique_integrals = unique_integrals[(unique_integrals &gt;= 0) &amp; (unique_integrals &lt;= len(grid_points))] bincount_left = tf.roll(tf.concat(tf.math.bincount(tf.cast(integral, tf.int32), weights=frac_weights), tf.constant(0)), shift=1, axis=0) bincount_right = tf.math.bincount(tf.cast(integral, tf.int32), weights=neg_frac_weights) bincount = tf.add(bincount_left, bincount_right) return bincount def _generate_fft_grid(self, num_points): grid_min = tf.math.reduce_min(self._data) grid_max = tf.math.reduce_max(self._data) num_intervals = tf.math.subtract(num_points, tf.constant(1)) dx = tf.math.divide(tf.math.subtract(grid_max, grid_min), tf.cast(num_intervals, tf.float32)) return tf.linspace(tf.math.multiply(tf.math.negative(dx), num_points), tf.math.multiply(dx, num_points), tf.math.add(tf.math.multiply(tf.cast(num_points, tf.int32), tf.constant(2)), tf.constant(1))) def _evaluate_fft(self, x): # Reshape in preparation to #! Not implemented yet! return super(KernelDensityEstimation, self)._log_prob(x) &quot;&quot;&quot;num_points = tf.size(self._grid) x = tensor_util.convert_nonref_to_tensor(x, name=&#39;x&#39;, dtype=tf.float32) l = tf.linspace(tf.math.multiply(tf.math.negative(dx), num_points), tf.math.multiply(dx, num_points), tf.math.add(tf.math.multiply(tf.cast(num_points, tf.int32), tf.constant(2)), tf.constant(1))) components_distribution_generator = lambda loc, scale: Independent(kernel(loc=loc, scale=scale)) n = x.shape[0] mixture_distribution=Categorical(probs=[1/n] * n) components_distribution=components_distribution_generator(loc=x, scale=self._bandwidth) super(KernelDensityEstimation, self).__init__( mixture_distribution=mixture_distribution, components_distribution=components_distribution) super(KernelDensityEstimation, self)._log_prob(x) kernel_weights = super(KernelDensityEstimation, self)._log_prob(x) zeros_count = tf.cast(tf.math.divide(tf.math.subtract(kernel_weights.shape[0], x.shape[0]), tf.constant(2)), tf.int32) paddings = paddings = [[zeros_count, zeros_count]] data = tf.pad(x, paddings, &quot;CONSTANT&quot;) tf.print(kernel_weights.shape) tf.print(data.shape) tf.print(data) # Use FFT kernel_weights = tf.signal.rfft(kernel_weights) data = tf.signal.rfft(data) return tf.signal.irfft(tf.math.multiply(data, kernel_weights))[zeros_count:-zeros_count]&quot;&quot;&quot; def _log_prob(self, x): if(self._use_fft): return self._evaluate_fft(x) else: return super(KernelDensityEstimation, self)._log_prob(x) "],
["comparison.html", "4 Comparison 4.1 Generation of Test Distribution 4.2 Comparing different methods", " 4 Comparison To compare the different implementations I created a simple test distribution comprised of three gaussian, one uniform and one exponential distribution. The distribution is created by using the TensorFlow Probability package and its Mixture Model. 4.1 Generation of Test Distribution Listing: Test Distribution generation import numpy as np import tensorflow as tf import tensorflow_probability as tfp import tensorflow_probability.python.distributions as tfd from tf_kde.distribution import KernelDensityEstimation r_seed = 1978239485 n_datapoints = 1000000 tfd = tfp.distributions mix_3gauss_1exp_1uni = tfd.Mixture( cat=tfd.Categorical(probs=[0.1, 0.2, 0.1, 0.4, 0.2]), components=[ tfd.Normal(loc=-1., scale=0.4), tfd.Normal(loc=+1., scale=0.5), tfd.Normal(loc=+1., scale=0.3), tfd.Exponential(rate=2), tfd.Uniform(low=-5, high=5) ]) data = mix_3gauss_1exp_1uni.sample(sample_shape=n_datapoints, seed=r_seed) data = data.numpy() 4.2 Comparing different methods import tensorflow as tf import numpy as np from KDEpy import FFTKDE from tf_kde.distribution import KernelDensityEstimation, KernelDensityEstimationBasic from zfit_benchmark.timer import Timer import seaborn as sns import pandas as pd from tf_kde.tests.test_distribution import data n_testpoints = 200 def kde_basic(data, x): fac = 1.0 / np.sqrt(2.0 * np.pi) exp_fac = -1.0/2.0 h = 0.01 y_fac = 1.0/(h*data.size) gauss_kernel = lambda x: fac * np.exp(exp_fac * x**2) y = np.zeros(x.size) for i, x_i in enumerate(x): y[i] = y_fac * np.sum(gauss_kernel((x_i-data)/h)) return y def kde_seaborn(data, x): sns.distplot(data, bins=1000, kde=True, rug=False) return np.NaN def kde_kdepy_fft(data, x): x = np.array(x) return FFTKDE(kernel=&quot;gaussian&quot;, bw=&quot;silverman&quot;).fit(data).evaluate(x) @tf.function(autograph=False) def kde_basic_tf_internal(data, x, n_datapoints): h1 = 0.01 fac = tf.constant(1.0 / np.sqrt(2.0 * np.pi), tf.float32) exp_fac = tf.constant(-1.0/2.0, tf.float32) y_fac = tf.constant(1.0/(h1 * n_datapoints), tf.float32) h = tf.constant(h1, tf.float32) gauss_kernel = lambda x: tf.math.multiply(fac, tf.math.exp(tf.math.multiply(exp_fac, tf.math.square(x)))) calc_value = lambda x: tf.math.multiply(y_fac, tf.math.reduce_sum(gauss_kernel(tf.math.divide(tf.math.subtract(x, data), h)))) return tf.map_fn(calc_value, x) def kde_basic_tf(data, x): n_datapoints = data.size return kde_basic_tf_internal(data, x, n_datapoints).numpy() def kde_tfp(data, x): dist = KernelDensityEstimationBasic(bandwidth=0.01, data=data) return dist.prob(x).numpy() def kde_tfp_mixture(data, x): dist = KernelDensityEstimation(bandwidth=0.01, data=data) return dist.prob(x).numpy() def kde_tfp_mixture_with_binned_data(data, x): dist = KernelDensityEstimation(bandwidth=0.01, data=data, use_grid=True) return dist.prob(x).numpy() def kde_tfp_mixture_with_fft(data, x): dist = KernelDensityEstimation(bandwidth=0.01, data=data, use_grid=True, use_fft=True) return dist.prob(x).numpy() methods = pd.DataFrame({ &#39;identifier&#39;: [ &#39;basic&#39;, &#39;seaborn&#39;, &#39;KDEpy&#39;, &#39;basicTF&#39;, &#39;tfp&#39;, &#39;tfpM&#39;, &#39;tfpMB&#39;, &#39;tfpMFFT&#39; ], &#39;label&#39;: [ &#39;Basic KDE with Python&#39;, &#39;KDE using seaborn.distplot&#39;, &#39;KDE using KDEpy.FFTKDE&#39;, &#39;Basic KDE in TensorFlow&#39;, &#39;KDE implemented as TensorFlow Probability Distribution Subclass&#39;, &#39;KDE implemented as TensorFlow Probability MixtureSameFamily Subclass&#39;, &#39;KDE implemented as TensorFlow Probability MixtureSameFamily Subclass with Binned Data&#39;, &#39;KDE implemented as TensorFlow Probability MixtureSameFamily Subclass with Fast Fourier Transform&#39; ], &#39;function&#39;:[ kde_basic, kde_seaborn, kde_kdepy_fft, kde_basic_tf, kde_tfp, kde_tfp_mixture, kde_tfp_mixture_with_binned_data, kde_tfp_mixture_with_fft ] }) methods.set_index(&#39;identifier&#39;, drop=False, inplace=True) estimations = pd.DataFrame() estimations[&#39;x&#39;] = np.linspace(-7.0, 7.0, num=n_testpoints, dtype=np.float32) methods[&#39;runtime&#39;] = np.NaN for index, method in methods.iterrows(): with Timer(&#39;Benchmarking&#39;) as timer: estimations[method[&#39;identifier&#39;]] = method[&#39;function&#39;](data, estimations[&#39;x&#39;]) timer.stop() methods.at[method[&#39;identifier&#39;], &#39;runtime&#39;] = timer.elapsed methods.drop(&#39;function&#39;, axis=1, inplace=True) Running this, leads to the following comparison: Table 4.1: Runtime comparison identifier label runtime basic basic Basic KDE with Python 2.8499737 seaborn seaborn KDE using seaborn.distplot 2.2177022 KDEpy KDEpy KDE using KDEpy.FFTKDE 0.0921045 basicTF basicTF Basic KDE in TensorFlow 1.3024085 tfp tfp KDE implemented as TensorFlow Probability Distribution Subclass 2.4570163 tfpM tfpM KDE implemented as TensorFlow Probability MixtureSameFamily Subclass 3.3410472 tfpMB tfpMB KDE implemented as TensorFlow Probability MixtureSameFamily Subclass with Binned Data 0.0704233 tfpMFFT tfpMFFT KDE implemented as TensorFlow Probability MixtureSameFamily Subclass with Fast Fourier Transform 0.0703308 Table 4.2: Accuracy comparison x basic seaborn KDEpy basicTF tfp tfpM tfpMB tfpMFFT -7.0000000 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.9296484 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.8592963 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.7889447 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.7185931 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.6482410 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.5778894 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.5075378 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.4371858 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.3668342 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.2964826 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.2261305 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.1557789 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.0854273 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -6.0150752 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.9447236 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.8743720 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.8040199 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.7336683 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.6633167 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.5929646 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.5226130 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.4522614 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.3819094 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.3115578 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.2412062 0.0000000 NaN 0.0000004 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.1708541 0.0000000 NaN 0.0000442 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.1005025 0.0000000 NaN 0.0009742 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 -5.0301509 0.0000293 NaN 0.0062010 0.0000293 0.0000293 0.0000293 0.0000548 0.0000548 -4.9597988 0.0208125 NaN 0.0158229 0.0208125 0.0208125 0.0208125 0.0207540 0.0207540 -4.8894472 0.0208162 NaN 0.0200963 0.0208162 0.0208162 0.0208162 0.0208610 0.0208610 -4.8190956 0.0204375 NaN 0.0206161 0.0204375 0.0204375 0.0204375 0.0205128 0.0205128 -4.7487435 0.0204646 NaN 0.0205411 0.0204646 0.0204646 0.0204646 0.0204957 0.0204957 -4.6783919 0.0205341 NaN 0.0204012 0.0205341 0.0205341 0.0205341 0.0204630 0.0204630 -4.6080403 0.0195643 NaN 0.0202287 0.0195643 0.0195643 0.0195643 0.0196219 0.0196219 -4.5376883 0.0210927 NaN 0.0201746 0.0210927 0.0210927 0.0210927 0.0209914 0.0209914 -4.4673367 0.0195981 NaN 0.0201938 0.0195981 0.0195981 0.0195981 0.0196680 0.0196680 -4.3969851 0.0204729 NaN 0.0204491 0.0204729 0.0204729 0.0204729 0.0205494 0.0205494 -4.3266330 0.0210287 NaN 0.0205885 0.0210287 0.0210287 0.0210287 0.0209845 0.0209845 -4.2562814 0.0198982 NaN 0.0204054 0.0198982 0.0198982 0.0198982 0.0200070 0.0200070 -4.1859298 0.0204449 NaN 0.0202258 0.0204449 0.0204449 0.0204449 0.0205717 0.0205717 -4.1155777 0.0204613 NaN 0.0201229 0.0204613 0.0204613 0.0204613 0.0203703 0.0203703 -4.0452261 0.0210005 NaN 0.0204305 0.0210005 0.0210005 0.0210006 0.0210243 0.0210243 -3.9748743 0.0194273 NaN 0.0204547 0.0194273 0.0194273 0.0194273 0.0195512 0.0195512 -3.9045227 0.0202804 NaN 0.0204616 0.0202804 0.0202804 0.0202804 0.0203311 0.0203311 -3.8341708 0.0199572 NaN 0.0202563 0.0199572 0.0199572 0.0199572 0.0200177 0.0200177 -3.7638190 0.0201123 NaN 0.0201723 0.0201123 0.0201123 0.0201123 0.0201145 0.0201145 -3.6934674 0.0209057 NaN 0.0202445 0.0209057 0.0209057 0.0209057 0.0209104 0.0209104 -3.6231155 0.0203726 NaN 0.0198083 0.0203726 0.0203726 0.0203726 0.0202654 0.0202654 -3.5527637 0.0193254 NaN 0.0192608 0.0193254 0.0193254 0.0193254 0.0192831 0.0192831 -3.4824121 0.0193953 NaN 0.0193589 0.0193953 0.0193953 0.0193953 0.0193969 0.0193969 -3.4120603 0.0200990 NaN 0.0198782 0.0200990 0.0200990 0.0200990 0.0201507 0.0201507 -3.3417087 0.0196300 NaN 0.0197611 0.0196300 0.0196300 0.0196299 0.0196535 0.0196535 -3.2713568 0.0180111 NaN 0.0194851 0.0180111 0.0180111 0.0180111 0.0180661 0.0180661 -3.2010050 0.0209866 NaN 0.0198947 0.0209866 0.0209866 0.0209866 0.0209217 0.0209217 -3.1306534 0.0192494 NaN 0.0199137 0.0192494 0.0192494 0.0192494 0.0192090 0.0192090 -3.0603015 0.0206995 NaN 0.0201499 0.0206995 0.0206995 0.0206995 0.0206156 0.0206156 -2.9899497 0.0202502 NaN 0.0204208 0.0202502 0.0202502 0.0202502 0.0202171 0.0202171 -2.9195981 0.0208145 NaN 0.0205284 0.0208145 0.0208145 0.0208145 0.0208098 0.0208098 -2.8492463 0.0200826 NaN 0.0202775 0.0200826 0.0200826 0.0200826 0.0201099 0.0201099 -2.7788944 0.0200118 NaN 0.0199981 0.0200118 0.0200118 0.0200118 0.0200466 0.0200466 -2.7085428 0.0199587 NaN 0.0197106 0.0199587 0.0199587 0.0199587 0.0199001 0.0199001 -2.6381910 0.0200374 NaN 0.0199455 0.0200374 0.0200374 0.0200374 0.0201217 0.0201217 -2.5678391 0.0199971 NaN 0.0201614 0.0199971 0.0199971 0.0199971 0.0200583 0.0200583 -2.4974875 0.0203591 NaN 0.0203501 0.0203591 0.0203591 0.0203591 0.0203331 0.0203331 -2.4271357 0.0208892 NaN 0.0206034 0.0208892 0.0208892 0.0208892 0.0209156 0.0209156 -2.3567839 0.0202368 NaN 0.0204627 0.0202368 0.0202368 0.0202368 0.0202893 0.0202893 -2.2864323 0.0214166 NaN 0.0206318 0.0214166 0.0214166 0.0214166 0.0213925 0.0213925 -2.2160804 0.0206180 NaN 0.0211450 0.0206180 0.0206180 0.0206180 0.0206368 0.0206368 -2.1457286 0.0221667 NaN 0.0219538 0.0221667 0.0221667 0.0221667 0.0221991 0.0221991 -2.0753770 0.0230426 NaN 0.0229285 0.0230426 0.0230426 0.0230426 0.0229957 0.0229957 -2.0050251 0.0235689 NaN 0.0242916 0.0235689 0.0235689 0.0235689 0.0237029 0.0237029 -1.9346733 0.0246321 NaN 0.0262133 0.0246321 0.0246321 0.0246321 0.0246177 0.0246177 -1.8643216 0.0304180 NaN 0.0298103 0.0304180 0.0304180 0.0304180 0.0301851 0.0301851 -1.7939699 0.0340315 NaN 0.0347089 0.0340315 0.0340315 0.0340315 0.0341411 0.0341411 -1.7236181 0.0398586 NaN 0.0404316 0.0398586 0.0398586 0.0398586 0.0397733 0.0397733 -1.6532663 0.0487671 NaN 0.0469134 0.0487671 0.0487671 0.0487671 0.0486207 0.0486207 -1.5829146 0.0540091 NaN 0.0542382 0.0540091 0.0540091 0.0540091 0.0538893 0.0538893 -1.5125629 0.0631989 NaN 0.0637595 0.0631988 0.0631988 0.0631988 0.0631624 0.0631624 -1.4422110 0.0729860 NaN 0.0745362 0.0729860 0.0729860 0.0729860 0.0731662 0.0731662 -1.3718593 0.0877974 NaN 0.0854362 0.0877974 0.0877974 0.0877974 0.0876047 0.0876047 -1.3015076 0.0932362 NaN 0.0946122 0.0932363 0.0932363 0.0932363 0.0934022 0.0934022 -1.2311558 0.1034280 NaN 0.1035766 0.1034280 0.1034280 0.1034280 0.1034043 0.1034043 -1.1608040 0.1111406 NaN 0.1113007 0.1111406 0.1111406 0.1111406 0.1111827 0.1111827 -1.0904523 0.1183688 NaN 0.1161050 0.1183688 0.1183688 0.1183688 0.1184133 0.1184133 -1.0201005 0.1154359 NaN 0.1170385 0.1154359 0.1154359 0.1154359 0.1157021 0.1157021 -0.9497488 0.1166944 NaN 0.1159430 0.1166944 0.1166944 0.1166943 0.1166564 0.1166564 -0.8793970 0.1136776 NaN 0.1130470 0.1136776 0.1136776 0.1136776 0.1137577 0.1137577 -0.8090453 0.1095859 NaN 0.1075309 0.1095859 0.1095859 0.1095859 0.1096082 0.1096082 -0.7386935 0.0972383 NaN 0.0990830 0.0972383 0.0972383 0.0972383 0.0974528 0.0974528 -0.6683417 0.0900137 NaN 0.0905486 0.0900137 0.0900137 0.0900136 0.0900610 0.0900610 -0.5979900 0.0822598 NaN 0.0815774 0.0822598 0.0822598 0.0822598 0.0823508 0.0823508 -0.5276382 0.0711820 NaN 0.0718010 0.0711820 0.0711820 0.0711820 0.0710989 0.0710989 -0.4572864 0.0632002 NaN 0.0629390 0.0632002 0.0632002 0.0632001 0.0630253 0.0630253 -0.3869347 0.0542016 NaN 0.0552546 0.0542016 0.0542016 0.0542016 0.0542900 0.0542900 -0.3165829 0.0479771 NaN 0.0491212 0.0479771 0.0479771 0.0479771 0.0479605 0.0479605 -0.2462312 0.0458891 NaN 0.0453334 0.0458891 0.0458891 0.0458891 0.0458408 0.0458408 -0.1758794 0.0430577 NaN 0.0445461 0.0430577 0.0430577 0.0430577 0.0429996 0.0429996 -0.1055276 0.0409512 NaN 0.0718943 0.0409512 0.0409512 0.0409512 0.0410765 0.0410765 -0.0351759 0.0454857 NaN 0.2440335 0.0454857 0.0454857 0.0454857 0.0459486 0.0459486 0.0351759 0.7873049 NaN 0.5694473 0.7873048 0.7873048 0.7873043 0.7873461 0.7873461 0.1055276 0.7071357 NaN 0.6781591 0.7071357 0.7071357 0.7071356 0.7064998 0.7064998 0.1758794 0.6368205 NaN 0.6331000 0.6368204 0.6368204 0.6368200 0.6366546 0.6366546 0.2462312 0.5640191 NaN 0.5700303 0.5640189 0.5640189 0.5640186 0.5644068 0.5644068 0.3165829 0.5182795 NaN 0.5208183 0.5182794 0.5182794 0.5182794 0.5184755 0.5184755 0.3869347 0.4786080 NaN 0.4844219 0.4786080 0.4786080 0.4786079 0.4789822 0.4789822 0.4572864 0.4555878 NaN 0.4579264 0.4555878 0.4555878 0.4555877 0.4553497 0.4553497 0.5276382 0.4363624 NaN 0.4408690 0.4363622 0.4363622 0.4363621 0.4362947 0.4362947 0.5979900 0.4315099 NaN 0.4337052 0.4315099 0.4315099 0.4315100 0.4311112 0.4311112 0.6683417 0.4341171 NaN 0.4324081 0.4341172 0.4341172 0.4341170 0.4342930 0.4342930 0.7386935 0.4266405 NaN 0.4326203 0.4266404 0.4266404 0.4266402 0.4280218 0.4280218 0.8090453 0.4356889 NaN 0.4335869 0.4356889 0.4356889 0.4356889 0.4349992 0.4349992 0.8793970 0.4376814 NaN 0.4330233 0.4376814 0.4376814 0.4376813 0.4377266 0.4377266 0.9497488 0.4280117 NaN 0.4254692 0.4280116 0.4280116 0.4280115 0.4276654 0.4276654 1.0201005 0.4144693 NaN 0.4121747 0.4144693 0.4144693 0.4144693 0.4147335 0.4147335 1.0904523 0.4017844 NaN 0.3923837 0.4017844 0.4017844 0.4017845 0.4016601 0.4016601 1.1608040 0.3672819 NaN 0.3637526 0.3672818 0.3672818 0.3672817 0.3669998 0.3669998 1.2311558 0.3299960 NaN 0.3302301 0.3299960 0.3299960 0.3299959 0.3300476 0.3300476 1.3015076 0.2935527 NaN 0.2935988 0.2935527 0.2935527 0.2935526 0.2932151 0.2932151 1.3718593 0.2527414 NaN 0.2557451 0.2527415 0.2527415 0.2527415 0.2533529 0.2533529 1.4422110 0.2193653 NaN 0.2201038 0.2193653 0.2193653 0.2193652 0.2193426 0.2193426 1.5125629 0.1854102 NaN 0.1873280 0.1854101 0.1854101 0.1854101 0.1855599 0.1855599 1.5829146 0.1594451 NaN 0.1573636 0.1594451 0.1594451 0.1594451 0.1591008 0.1591008 1.6532663 0.1295867 NaN 0.1310772 0.1295867 0.1295867 0.1295867 0.1295274 0.1295274 1.7236181 0.1085992 NaN 0.1096769 0.1085992 0.1085992 0.1085991 0.1088532 0.1088532 1.7939699 0.0910477 NaN 0.0920858 0.0910477 0.0910477 0.0910477 0.0912394 0.0912394 1.8643216 0.0747190 NaN 0.0774625 0.0747190 0.0747190 0.0747190 0.0749482 0.0749482 1.9346733 0.0644320 NaN 0.0658434 0.0644320 0.0644320 0.0644320 0.0643863 0.0643863 2.0050251 0.0548668 NaN 0.0561208 0.0548668 0.0548668 0.0548668 0.0549509 0.0549509 2.0753770 0.0483242 NaN 0.0481506 0.0483242 0.0483242 0.0483242 0.0483078 0.0483078 2.1457286 0.0417622 NaN 0.0422845 0.0417622 0.0417622 0.0417621 0.0418193 0.0418193 2.2160804 0.0364343 NaN 0.0378252 0.0364343 0.0364343 0.0364343 0.0364848 0.0364848 2.2864323 0.0342945 NaN 0.0342508 0.0342945 0.0342945 0.0342944 0.0342812 0.0342812 2.3567839 0.0319098 NaN 0.0314653 0.0319098 0.0319098 0.0319098 0.0318336 0.0318336 2.4271357 0.0299350 NaN 0.0292150 0.0299350 0.0299350 0.0299350 0.0298547 0.0298547 2.4974875 0.0276999 NaN 0.0275424 0.0276999 0.0276999 0.0276999 0.0276421 0.0276421 2.5678391 0.0269859 NaN 0.0265272 0.0269859 0.0269859 0.0269859 0.0269832 0.0269832 2.6381910 0.0255748 NaN 0.0252025 0.0255748 0.0255748 0.0255748 0.0255706 0.0255706 2.7085428 0.0238882 NaN 0.0238795 0.0238882 0.0238882 0.0238882 0.0238929 0.0238929 2.7788944 0.0237519 NaN 0.0232494 0.0237519 0.0237519 0.0237519 0.0237675 0.0237675 2.8492463 0.0227718 NaN 0.0230655 0.0227718 0.0227718 0.0227718 0.0227749 0.0227749 2.9195981 0.0231769 NaN 0.0230343 0.0231769 0.0231769 0.0231769 0.0232638 0.0232638 2.9899497 0.0221606 NaN 0.0224712 0.0221606 0.0221606 0.0221607 0.0222441 0.0222441 3.0603015 0.0228210 NaN 0.0219217 0.0228210 0.0228210 0.0228210 0.0226709 0.0226709 3.1306534 0.0202293 NaN 0.0213619 0.0202293 0.0202293 0.0202293 0.0202368 0.0202368 3.2010050 0.0209242 NaN 0.0211865 0.0209242 0.0209242 0.0209242 0.0209736 0.0209736 3.2713568 0.0208098 NaN 0.0209442 0.0208098 0.0208098 0.0208098 0.0208819 0.0208819 3.3417087 0.0189643 NaN 0.0207521 0.0189643 0.0189643 0.0189643 0.0191196 0.0191196 3.4120603 0.0211910 NaN 0.0210935 0.0211910 0.0211910 0.0211909 0.0213489 0.0213489 3.4824121 0.0210251 NaN 0.0209809 0.0210251 0.0210251 0.0210251 0.0209947 0.0209947 3.5527637 0.0203314 NaN 0.0207411 0.0203314 0.0203314 0.0203314 0.0202540 0.0202540 3.6231155 0.0208459 NaN 0.0206224 0.0208459 0.0208459 0.0208459 0.0209168 0.0209168 3.6934674 0.0203483 NaN 0.0202520 0.0203483 0.0203483 0.0203483 0.0203373 0.0203373 3.7638190 0.0205136 NaN 0.0200463 0.0205136 0.0205136 0.0205136 0.0205522 0.0205522 3.8341708 0.0193994 NaN 0.0197578 0.0193994 0.0193994 0.0193994 0.0194468 0.0194468 3.9045227 0.0188916 NaN 0.0195506 0.0188916 0.0188916 0.0188916 0.0189055 0.0189055 3.9748743 0.0204086 NaN 0.0200201 0.0204086 0.0204086 0.0204086 0.0203007 0.0203007 4.0452261 0.0207592 NaN 0.0204103 0.0207592 0.0207592 0.0207592 0.0207776 0.0207776 4.1155777 0.0197156 NaN 0.0201801 0.0197156 0.0197156 0.0197156 0.0197681 0.0197681 4.1859298 0.0195413 NaN 0.0199606 0.0195412 0.0195412 0.0195412 0.0195613 0.0195613 4.2562814 0.0207398 NaN 0.0201412 0.0207398 0.0207398 0.0207398 0.0207392 0.0207392 4.3266330 0.0197963 NaN 0.0201330 0.0197963 0.0197963 0.0197963 0.0197747 0.0197747 4.3969851 0.0199690 NaN 0.0200822 0.0199690 0.0199690 0.0199690 0.0199235 0.0199235 4.4673367 0.0199505 NaN 0.0202356 0.0199505 0.0199505 0.0199505 0.0199084 0.0199084 4.5376883 0.0214757 NaN 0.0204888 0.0214757 0.0214757 0.0214757 0.0213867 0.0213867 4.6080403 0.0201935 NaN 0.0203624 0.0201935 0.0201935 0.0201935 0.0201880 0.0201880 4.6783919 0.0212064 NaN 0.0201697 0.0212064 0.0212064 0.0212064 0.0211177 0.0211177 4.7487435 0.0195216 NaN 0.0198779 0.0195216 0.0195216 0.0195216 0.0195151 0.0195151 4.8190956 0.0205175 NaN 0.0200590 0.0205175 0.0205175 0.0205175 0.0203096 0.0203096 4.8894472 0.0214562 NaN 0.0201341 0.0214562 0.0214562 0.0214562 0.0213388 0.0213388 4.9597988 0.0210350 NaN 0.0160540 0.0210350 0.0210350 0.0210350 0.0210256 0.0210256 5.0301509 0.0001065 NaN 0.0063106 0.0001065 0.0001065 0.0001065 0.0001405 0.0001405 5.1005025 0.0000001 NaN 0.0010112 0.0000001 0.0000001 0.0000001 0.0000006 0.0000006 5.1708541 0.0000007 NaN 0.0000669 0.0000007 0.0000007 0.0000007 0.0000016 0.0000016 5.2412062 0.0000058 NaN 0.0000208 0.0000058 0.0000058 0.0000058 0.0000082 0.0000082 5.3115578 0.0000186 NaN 0.0000228 0.0000186 0.0000186 0.0000186 0.0000211 0.0000211 5.3819094 0.0000308 NaN 0.0000197 0.0000308 0.0000308 0.0000308 0.0000287 0.0000287 5.4522614 0.0000158 NaN 0.0000137 0.0000158 0.0000158 0.0000158 0.0000178 0.0000178 5.5226130 0.0000031 NaN 0.0000118 0.0000031 0.0000031 0.0000031 0.0000049 0.0000049 5.5929646 0.0000000 NaN 0.0000131 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 5.6633167 0.0000038 NaN 0.0000198 0.0000038 0.0000038 0.0000038 0.0000064 0.0000064 5.7336683 0.0000027 NaN 0.0000216 0.0000027 0.0000027 0.0000027 0.0000043 0.0000043 5.8040199 0.0000393 NaN 0.0000167 0.0000393 0.0000393 0.0000393 0.0000391 0.0000391 5.8743720 0.0000000 NaN 0.0000088 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 5.9447236 0.0000032 NaN 0.0000149 0.0000032 0.0000032 0.0000032 0.0000039 0.0000039 6.0150752 0.0000237 NaN 0.0000178 0.0000237 0.0000237 0.0000237 0.0000243 0.0000243 6.0854273 0.0000000 NaN 0.0000104 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 6.1557789 0.0000105 NaN 0.0000075 0.0000105 0.0000105 0.0000105 0.0000105 0.0000105 6.2261305 0.0000000 NaN 0.0000043 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 6.2964826 0.0000011 NaN 0.0000061 0.0000011 0.0000011 0.0000011 0.0000012 0.0000012 6.3668342 0.0000000 NaN 0.0000049 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 6.4371858 0.0000000 NaN 0.0000013 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 6.5075378 0.0000000 NaN 0.0000001 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 6.5778894 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 6.6482410 0.0000000 NaN 0.0000008 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 6.7185931 0.0000000 NaN 0.0000042 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 6.7889447 0.0000099 NaN 0.0000067 0.0000099 0.0000099 0.0000099 0.0000099 0.0000099 6.8592963 0.0000000 NaN 0.0000023 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 6.9296484 0.0000000 NaN 0.0000001 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 7.0000000 0.0000000 NaN 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 Plotted for reference: "],
["summary.html", "5 Summary", " 5 Summary In summary we can conclude that… "],
["references.html", "References", " References "]
]
