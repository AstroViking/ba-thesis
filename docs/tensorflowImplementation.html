<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Implementation | Performance of Univariate Kernel Density Estimation methods in TensorFlow</title>
  <meta name="description" content="3 Implementation | Performance of Univariate Kernel Density Estimation methods in TensorFlow" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Implementation | Performance of Univariate Kernel Density Estimation methods in TensorFlow" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Implementation | Performance of Univariate Kernel Density Estimation methods in TensorFlow" />
  
  
  

<meta name="author" content="Marc Steiner" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="currentState.html"/>
<link rel="next" href="comparison.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#kernel-density-estimation"><i class="fa fa-check"></i><b>1.1</b> Kernel Density Estimation</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#zfit-and-tensorflow"><i class="fa fa-check"></i><b>1.2</b> zfit and TensorFlow</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="currentState.html"><a href="currentState.html"><i class="fa fa-check"></i><b>2</b> Current state of the art</a></li>
<li class="chapter" data-level="3" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html"><i class="fa fa-check"></i><b>3</b> Implementation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html#exact-kernel-density-estimation"><i class="fa fa-check"></i><b>3.1</b> Exact Kernel Density Estimation</a></li>
<li class="chapter" data-level="3.2" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html#simple-and-linear-binning"><i class="fa fa-check"></i><b>3.2</b> Simple and linear Binning</a></li>
<li class="chapter" data-level="3.3" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html#using-convolution-and-the-fast-fourier-transform"><i class="fa fa-check"></i><b>3.3</b> Using convolution and the Fast Fourier Transform</a></li>
<li class="chapter" data-level="3.4" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html#improved-sheather-jones-algorithm"><i class="fa fa-check"></i><b>3.4</b> Improved Sheather Jones Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="comparison.html"><a href="comparison.html"><i class="fa fa-check"></i><b>4</b> Comparison</a>
<ul>
<li class="chapter" data-level="4.1" data-path="comparison.html"><a href="comparison.html#benchmark-setup"><i class="fa fa-check"></i><b>4.1</b> Benchmark setup</a></li>
<li class="chapter" data-level="4.2" data-path="comparison.html"><a href="comparison.html#basic-implementation-against-binned-and-fft-implementations"><i class="fa fa-check"></i><b>4.2</b> Basic implementation against Binned and FFT implementations</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="comparison.html"><a href="comparison.html#accuracy"><i class="fa fa-check"></i><b>4.2.1</b> Accuracy</a></li>
<li class="chapter" data-level="4.2.2" data-path="comparison.html"><a href="comparison.html#runtime"><i class="fa fa-check"></i><b>4.2.2</b> Runtime</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="comparison.html"><a href="comparison.html#new-implementation-against-kdepy"><i class="fa fa-check"></i><b>4.3</b> New implementation against KDEpy</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="comparison.html"><a href="comparison.html#accuracy-1"><i class="fa fa-check"></i><b>4.3.1</b> Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="comparison.html"><a href="comparison.html#runtime-1"><i class="fa fa-check"></i><b>4.3.2</b> Runtime</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="comparison.html"><a href="comparison.html#new-implementation-run-with-gpu-support"><i class="fa fa-check"></i><b>4.4</b> New implementation run with GPU support</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="comparison.html"><a href="comparison.html#accuracy-2"><i class="fa fa-check"></i><b>4.4.1</b> Accuracy</a></li>
<li class="chapter" data-level="4.4.2" data-path="comparison.html"><a href="comparison.html#runtime-2"><i class="fa fa-check"></i><b>4.4.2</b> Runtime</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Performance of Univariate Kernel Density Estimation methods in TensorFlow</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tensorflowImplementation" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Implementation</h1>
<div id="exact-kernel-density-estimation" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Exact Kernel Density Estimation</h2>
<p>The implementation of a simple Kernel Density Estimation in TensorFlow is straightforward. As described in the original Tensorflow Probability Paper<span class="citation"><sup><a href="references.html#ref-googleTFP" role="doc-biblioref">14</a></sup></span>, a KDE can be constructed by using its MixtureSameFamily Distribution, given sampled <code>data</code> as follows</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="tensorflowImplementation.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow_probability <span class="im">import</span> distributions <span class="im">as</span> tfd</span>
<span id="cb1-2"><a href="tensorflowImplementation.html#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="tensorflowImplementation.html#cb1-3" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: tfd.Independent(tfd.Normal(loc<span class="op">=</span>x, scale<span class="op">=</span><span class="fl">1.</span>))</span>
<span id="cb1-4"><a href="tensorflowImplementation.html#cb1-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> data.shape[<span class="dv">0</span>].value</span>
<span id="cb1-5"><a href="tensorflowImplementation.html#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="tensorflowImplementation.html#cb1-6" aria-hidden="true" tabindex="-1"></a>kde <span class="op">=</span> tfd.MixtureSameFamily(</span>
<span id="cb1-7"><a href="tensorflowImplementation.html#cb1-7" aria-hidden="true" tabindex="-1"></a>    mixture_distribution<span class="op">=</span>tfd.Categorical(</span>
<span id="cb1-8"><a href="tensorflowImplementation.html#cb1-8" aria-hidden="true" tabindex="-1"></a>        probs<span class="op">=</span>[<span class="dv">1</span> <span class="op">/</span> n] <span class="op">*</span> n),</span>
<span id="cb1-9"><a href="tensorflowImplementation.html#cb1-9" aria-hidden="true" tabindex="-1"></a>    components_distribution<span class="op">=</span>f(data))</span></code></pre></div>
<p>Interestingly, due to the smart encapsulated structure of TensorFlow Probability we can use any distribution of the loc-scale family type as a kernel, if there exists an implementation for it in TensorFlow Probability. If the used Kernel has only bounded support, the implementation proposed in this paper allows to specify the support upon instantiation of the class. If the Kernel has infinite support (like a Gaussian kernel for instance) a practical support estimate is calculated by searching for approximate roots with Brent’s method<span class="citation"><sup><a href="references.html#ref-brent1971algorithm" role="doc-biblioref">15</a></sup></span> implemented for TensorFlow in the python package <code>tf_quant_finance</code> by Google. This allows us to speed up the calculation.</p>
<p>However calculating an exact kernel density estimation is not always feasible as this can take a long time with a huge collection of events, especially in high energy physics. By implementing it in TensorFlow we already get a significant speed up compared to implementations in native Python, since most of TensorFlow is actually implemented in C++ and the code is optimized before running. But the computational complexity however, remains the same of course.</p>
<p><span class="math display" id="eq:kde">\[\begin{equation}
\widehat{f}_h(x) = \frac{1}{nh} \sum_{k=1}^n K\Big(\frac{x-x_k}{h}\Big)
\tag{3.1}
\end{equation}\]</span></p>
<p>The computational complexity of the basic exact KDE above is <span class="math inline">\(\mathcal{O}(nm)\)</span> where <span class="math inline">\(n\)</span> is the number of sample points to estimate from and <span class="math inline">\(m\)</span> is the number of evaluation points (the points where you want to calculate the estimate).</p>
<p>To combat this complexity several methods exist.</p>
</div>
<div id="simple-and-linear-binning" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Simple and linear Binning</h2>
<p>The most straightforward way to decrease runtime is by limiting the number of sample points. This can be done by a binning routine, where the values at a smaller number of regular grid points are estimated from the original large number of sample points.
Given a set of sample points <span class="math inline">\(X = \{x_0, x_1, ..., x_k, ..., x_{n-1}, x_n\}\)</span> with weights <span class="math inline">\(w_k\)</span> and a set of equally spaced grid points <span class="math inline">\(G = \{g_0, g_1, ..., g_l, ..., g_{n-1}, g_M\}\)</span> where <span class="math inline">\(N &lt; n\)</span> we can assign an estimate (or a count) <span class="math inline">\(c_l\)</span> to each grid point <span class="math inline">\(g_l\)</span> and use the newly found <span class="math inline">\(g_l\)</span>’s to calculate the kernel density estimation instead. This brings the computational complexity down to <span class="math inline">\(\mathcal{O}(Nm)\)</span>. Depending on the number of grid points <span class="math inline">\(N\)</span> the estimate is either more accurate and slower or less accurate and faster. However as we will see in the comparison chapter later as well, even a grid of size <span class="math inline">\(1024\)</span> is enough to capture the true density with high accuracy on a million data points.<span class="citation"><sup><a href="references.html#ref-KDEpyDoc" role="doc-biblioref">13</a></sup></span>.</p>
<p>As described in the excellent overview by Artur Gramacki<span class="citation"><sup><a href="references.html#ref-gramacki2018fft" role="doc-biblioref">16</a></sup></span> simple binning or linear binning can be used, although the last is often preferred since it is more accurate and the difference in computational complexity is negligible.</p>
<p>Simple binning is just the standard process of taking a weighted histogram that is divided by the sum of the sample points weights (normalization). In one dimension simple binning is binary in that it assigns a data points weight (<span class="math inline">\(1\)</span> for an unweighted histogram) either to the grid point (bin) left or right of itself. Linear binning on the other hand assigns a fraction of the whole weight to both grid points (bins) on either side, proportional to the closeness of grid point and data point in relation to the distance between grid points (bin width).</p>
<p>Mathematically linear binning in one dimension can be calculated like this:</p>
<p><span class="math display" id="eq:linbin">\[\begin{equation}
c_l = c(g_l) = \sum_{\substack{x_k \in X\\g_l &lt; x_k &lt; g_{l+1}}} \frac{g_{k+1}-x_k}{g_{l+1} - g_l} \cdot w_k + \sum_{\substack{x_k \in X\\g_{l-1} &lt; x_k &lt; g_l}} \frac{x_k - g_{l-1}}{g_{l+1} - g_l} \cdot w_k
\tag{3.2}
\end{equation}\]</span></p>
<p>Implementing linear binning efficiently with TensorFlow is a bit tricky since loops should be avoided. However with some inspiration from the excellent KDEpy package<span class="citation"><sup><a href="references.html#ref-KDEpy" role="doc-biblioref">11</a></sup></span> this can be done without using loops at all. By transforming the data such that every data point <span class="math inline">\(x_k\)</span> can be described by an integral part (corresponding to its nearest left grid point number <span class="math inline">\(l\)</span>) plus some fractional part (corresponding to the distance between grid point <span class="math inline">\(g_l\)</span> and data point <span class="math inline">\(x_k\)</span>) and applying <code>tf.math.bincount</code> twice to the transformed integral part weighting it with the fractional part times the initial weight.</p>
<p>The kernel density estimation can then be calculated as a mixture distribution of kernels located at the grid points, weighted with their associated grid count.</p>
<p><span class="math display" id="eq:kdebin">\[\begin{equation}
\widehat{f}_h(x) = \frac{1}{nh} \sum_{l=1}^N c_l \cdot K\Big(\frac{x-g_l}{h}\Big)
\tag{3.3}
\end{equation}\]</span></p>
</div>
<div id="using-convolution-and-the-fast-fourier-transform" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Using convolution and the Fast Fourier Transform</h2>
<p>With binning implemented, another technique to speed up the computation is rewriting the Kernel Density Estimation as convolution operation between the kernel and the grid counts calculated by the binning routine. By using the fact that a convolution is just a multiplication in Fourier space one can reduce the computational complexity down to <span class="math inline">\(\mathcal{O}(\log{N} \cdot m)\)</span>.<span class="citation"><sup><a href="references.html#ref-gramacki2018fft" role="doc-biblioref">16</a></sup></span></p>
<p>Using the equation <a href="tensorflowImplementation.html#eq:kdebin">(3.3)</a> from above but also only evaluating it at grid points gives us</p>
<p><span class="math display" id="eq:binkdegrid">\[\begin{equation}
\widehat{f}_h(g_j) = \frac{1}{nh} \sum_{l=1}^N c_l \cdot K\Big(\frac{g_j-g_l}{h}\Big) = \frac{1}{nh} \sum_{l=1}^N k_{j-l} \cdot c_l
\tag{3.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(k_{j-l} = K(\frac{g_j-g_l}{h})\)</span>.
If we set <span class="math inline">\(c_l = 0\)</span> for all <span class="math inline">\(l\)</span> not in the set <span class="math inline">\(\{1, ..., N\}\)</span> and notice that <span class="math inline">\(K(-x) = K(x)\)</span> we can extend equation <a href="tensorflowImplementation.html#eq:binkdegrid">(3.4)</a> to a discrete convolution as follows</p>
<p><span class="math display" id="eq:binkdeconv">\[\begin{equation}
\widehat{f}_h(g_j) = \frac{1}{nh} \sum_{l=-N}^N k_{j-l} \cdot c_l = \vec{c} \ast \vec{k}
\tag{3.5}
\end{equation}\]</span></p>
<p>where the two vectors look like this</p>
<div class="figure"><span id="fig:ckFigure"></span>
<img src="figures/c_conv_k.png" alt="Vectors $\vec{c}$ and $\vec{k}$"  />
<p class="caption">
Figure 3.1: Vectors <span class="math inline">\(\vec{c}\)</span> and <span class="math inline">\(\vec{k}\)</span>
</p>
</div>
<p>By using the well known convolution theorem we can fourier transform <span class="math inline">\(\vec{c}\)</span> and <span class="math inline">\(\vec{k}\)</span> multiply them and inverse fourier transform them back into real space.</p>
<p>In TensorFlow convolutions are efficiently implemented already in this way if we use <code>tf.nn.conv1d</code>. In benchmarking using this method proved significantly faster than using <code>tf.signal.rfft</code> and <code>tf.signal.irfft</code> to transform, multiply and inverse transform the vectors, which is implemented as an alternative as well.</p>
<p>This algorithm is implemented as its own class since it does not represent a complete mixture distribution anymore but calculates just the density distribution values at the speficied grid points. To still infer values for other points in the range of <span class="math inline">\(x\)</span> <code>tfp.math.interp_regular_1d_grid</code> is used which computes a linear interpolation of values between the grid.</p>
</div>
<div id="improved-sheather-jones-algorithm" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Improved Sheather Jones Algorithm</h2>
<p>A different take on Kernel Density Estimators is described in the paper ‘Kernel density estimation by diffusion’ by Botev et al.<span class="citation"><sup><a href="references.html#ref-botev2010kernel" role="doc-biblioref">12</a></sup></span>. The authors present a new adaptive kernel density estimator based on linear diffusion processes which also includes an estimation for the optimal bandwidth.</p>
<p>The algorithm is quite difficult to understand, a detailed explanation is given in the ‘Handbook of Monte Carlo Methods’<span class="citation"><sup><a href="references.html#ref-kroese2013handbook" role="doc-biblioref">17</a></sup></span> by the original paper authors. However the general idea is briefly sketched below.</p>
<p>A critical insight is that the Gaussian kernel density estimator <span class="math inline">\(\widehat{f}_{h,norm}\)</span> is the solution of the partial differential equation</p>
<p><span class="math display" id="eq:heatpde">\[\begin{equation}
\frac{\partial}{\partial t} \widehat{f}_{h,norm}(x,t) = \frac{1}
{2} \frac{\partial^2}{\partial x^2} \widehat{f}_{h,norm}(x,t),\text{ } t&gt;0 
\tag{3.6}
\end{equation}\]</span></p>
<p>with <span class="math inline">\(x \in \mathbb{R}\)</span>, <span class="math inline">\(\lim_{x\rightarrow \pm \infty}{\widehat{f}_{h,norm}(x,t) = 0}\)</span> and initial condition <span class="math inline">\(\widehat{f}_{h,norm}(x,0) = \Delta(x)\)</span>, where <span class="math inline">\(\Delta(x) = \frac{1}{N} \sum_{k=0}^N \delta_{x_k}(x)\)</span> is the empirical density of the given sample points <span class="math inline">\(X = \{x_0, x_1, ..., x_k, ..., x_{n-1}, x_n\}\)</span> and <span class="math inline">\(\delta_{x_k}(x)\)</span> is the Dirac measure at <span class="math inline">\(x_k\)</span>.</p>
<p>This means the kernel density estimator can be obtained by evolving the solution of the partial differential equation <a href="tensorflowImplementation.html#eq:heatpde">(3.6)</a> up to time <span class="math inline">\(t\)</span>. The key observation is that <a href="tensorflowImplementation.html#eq:heatpde">(3.6)</a> can be solved on a finite domain efficiently using the fast cosine transform - an FFT-related transform.<span class="citation"><sup><a href="references.html#ref-kroese2013handbook" role="doc-biblioref">17</a></sup></span></p>
<p>The optimal bandwidth is often defined as the one that minimizes the mean integrated square error(<span class="math inline">\(MISE\)</span>)</p>
<p><span class="math display" id="eq:mise">\[\begin{equation}
MISE(t) = \mathbb{E}_f\int [\widehat{f}_{h,norm}(x,t) - f(x)]^2 dx
\tag{3.7}
\end{equation}\]</span></p>
<p>An asymptotically optimal value <span class="math inline">\(t^{\ast}\)</span> which minimizes a first-order asymptotic appoximation of the <span class="math inline">\(MISE\)</span> is then given by<span class="citation"><sup><a href="references.html#ref-kroese2013handbook" role="doc-biblioref">17</a></sup></span></p>
<p><span class="math display" id="eq:tstar">\[\begin{equation}
t^{\ast} = \Big( \frac{1}{2N\sqrt{\pi} \| f&#39;&#39;\|^2}\Big)^{\frac{2}{5}}
\tag{3.8}
\end{equation}\]</span></p>
<p>Using the fact that <span class="math inline">\(\|f^{(j)}\|^2 = (-1)^j \mathbb{E}_f[f^{(2j)}(X)], \text{ } j\geq 1\)</span> and an initial estimation for <span class="math inline">\(\|\widehat{f}_{h,norm}^{(l+2)}\|^2\)</span> for some <span class="math inline">\(l \geq 3\)</span> one can then iteratively get a an estimation for <span class="math inline">\(\|\widehat{f}_{h,norm}^{(2)}\|^2\)</span> which can then be used to estimate <span class="math inline">\(t^{\ast}\)</span> instead of <span class="math inline">\(\|f&#39;&#39;\|^2\)</span>. According to their handbook <span class="math inline">\(l = 7\)</span> is a suitable value to yield good practical results.</p>
<p>The improvment compared to the standard Sheather-Jones plug-on method<span class="citation"><sup><a href="references.html#ref-sheather1991reliable" role="doc-biblioref">18</a></sup></span> consists in the fact that the initial estimation of <span class="math inline">\(\|\widehat{f}_{h,norm}^{(l+2)}\|^2\)</span> is calculated by solving the partial differental equation using the fast cosine transform as described above, eliminating the need to assume normally distributed data for the initial estimate and leading to improved performance, especially for density distributions that are far from normal as seen in the next chapter.</p>
<p>The implementation of the algorithm in TensorFlow proposed in this paper was also inspired a lot by the python package KDEpy<span class="citation"><sup><a href="references.html#ref-KDEpy" role="doc-biblioref">11</a></sup></span> and uses Brent’s method<span class="citation"><sup><a href="references.html#ref-brent1971algorithm" role="doc-biblioref">15</a></sup></span> to find roots, implemented in TensorFlow in the python package <code>tf_quant_finance</code>.</p>
<p>One shortcoming of the improved Sheather-Jones algorithm (ISJ) is that with few data points to estimate from, it is not guarenteed to converge. If that happens, one has to use the exact, binned or FFT kernel density estimators as described above.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="currentState.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="comparison.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/AstroViking/ba-thesis/edit/master/chapters/03-implementation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/AstroViking/ba-thesis/blob/master/chapters/03-implementation.Rmd",
"text": null
},
"download": ["thesis.pdf", "thesis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
