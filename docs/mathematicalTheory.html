<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Theory | Performance of Univariate Kernel Density Estimation methods in TensorFlow</title>
  <meta name="description" content="2 Theory | Performance of Univariate Kernel Density Estimation methods in TensorFlow" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Theory | Performance of Univariate Kernel Density Estimation methods in TensorFlow" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Theory | Performance of Univariate Kernel Density Estimation methods in TensorFlow" />
  
  
  

<meta name="author" content="Marc Steiner" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="currentState.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#kernel-density-estimation"><i class="fa fa-check"></i><b>1.1</b> Kernel Density Estimation</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#zfit-and-tensorflow"><i class="fa fa-check"></i><b>1.2</b> zfit and TensorFlow</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#purpose-of-this-thesis"><i class="fa fa-check"></i><b>1.3</b> Purpose of this thesis</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#introduction-univariate"><i class="fa fa-check"></i><b>1.4</b> Univariate case</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="mathematicalTheory.html"><a href="mathematicalTheory.html"><i class="fa fa-check"></i><b>2</b> Theory</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mathematicalTheory.html"><a href="mathematicalTheory.html#exact-kernel-density-estimation"><i class="fa fa-check"></i><b>2.1</b> Exact Kernel Density Estimation</a></li>
<li class="chapter" data-level="2.2" data-path="mathematicalTheory.html"><a href="mathematicalTheory.html#simple-and-linear-binning"><i class="fa fa-check"></i><b>2.2</b> Simple and linear binning</a></li>
<li class="chapter" data-level="2.3" data-path="mathematicalTheory.html"><a href="mathematicalTheory.html#using-convolution-and-the-fast-fourier-transform"><i class="fa fa-check"></i><b>2.3</b> Using convolution and the Fast Fourier Transform</a></li>
<li class="chapter" data-level="2.4" data-path="mathematicalTheory.html"><a href="mathematicalTheory.html#improved-sheather-jones-algorithm"><i class="fa fa-check"></i><b>2.4</b> Improved Sheather-Jones Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="currentState.html"><a href="currentState.html"><i class="fa fa-check"></i><b>3</b> Current state of the art</a></li>
<li class="chapter" data-level="4" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html"><i class="fa fa-check"></i><b>4</b> Implementation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html#exact-kernel-density-estimation-1"><i class="fa fa-check"></i><b>4.1</b> Exact Kernel Density Estimation</a></li>
<li class="chapter" data-level="4.2" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html#simple-and-linear-binning-1"><i class="fa fa-check"></i><b>4.2</b> Simple and linear Binning</a></li>
<li class="chapter" data-level="4.3" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html#using-convolution-and-the-fast-fourier-transform-1"><i class="fa fa-check"></i><b>4.3</b> Using convolution and the Fast Fourier Transform</a></li>
<li class="chapter" data-level="4.4" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html#improved-sheather-jones-algorithm-1"><i class="fa fa-check"></i><b>4.4</b> Improved Sheather Jones Algorithm</a></li>
<li class="chapter" data-level="4.5" data-path="tensorflowImplementation.html"><a href="tensorflowImplementation.html#source-code"><i class="fa fa-check"></i><b>4.5</b> Source Code</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="comparison.html"><a href="comparison.html"><i class="fa fa-check"></i><b>5</b> Comparison</a>
<ul>
<li class="chapter" data-level="5.1" data-path="comparison.html"><a href="comparison.html#benchmark-setup"><i class="fa fa-check"></i><b>5.1</b> Benchmark setup</a></li>
<li class="chapter" data-level="5.2" data-path="comparison.html"><a href="comparison.html#differences-of-exact-binned-fft-and-isj-implementations"><i class="fa fa-check"></i><b>5.2</b> Differences of Exact, Binned, FFT and ISJ implementations</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="comparison.html"><a href="comparison.html#accuracy"><i class="fa fa-check"></i><b>5.2.1</b> Accuracy</a></li>
<li class="chapter" data-level="5.2.2" data-path="comparison.html"><a href="comparison.html#runtime"><i class="fa fa-check"></i><b>5.2.2</b> Runtime</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="comparison.html"><a href="comparison.html#comparison-to-kdepy-on-cpu"><i class="fa fa-check"></i><b>5.3</b> Comparison to KDEpy on CPU</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="comparison.html"><a href="comparison.html#accuracy-1"><i class="fa fa-check"></i><b>5.3.1</b> Accuracy</a></li>
<li class="chapter" data-level="5.3.2" data-path="comparison.html"><a href="comparison.html#runtime-1"><i class="fa fa-check"></i><b>5.3.2</b> Runtime</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="comparison.html"><a href="comparison.html#comparison-to-kdepy-on-gpu"><i class="fa fa-check"></i><b>5.4</b> Comparison to KDEpy on GPU</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="comparison.html"><a href="comparison.html#accuracy-2"><i class="fa fa-check"></i><b>5.4.1</b> Accuracy</a></li>
<li class="chapter" data-level="5.4.2" data-path="comparison.html"><a href="comparison.html#runtime-2"><i class="fa fa-check"></i><b>5.4.2</b> Runtime</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>6</b> Summary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="presentation/index.html" target="_blank">Presentation</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Performance of Univariate Kernel Density Estimation methods in TensorFlow</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mathematicalTheory" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Theory</h1>
<div id="exact-kernel-density-estimation" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Exact Kernel Density Estimation</h2>
<p>An exact Kernel Density Estimation can be calculated as</p>
<p><span class="math display" id="eq:kde">\[\begin{equation}
\widehat{f}_h(x) = \frac{1}{nh} \sum_{k=1}^n K\Big(\frac{x-x_k}{h}\Big)
\tag{2.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(K(x)\)</span> is called the kernel function, which defines the range and size of influence of a single data point over the estimation. Most typically a simple Gaussian distribution is used as kernel function.
The wider the kernel function is, the farther is the influence of a single data point, this is characterized by the bandwidth paramter <span class="math inline">\(h\)</span>.</p>
<p>The computational complexity of the exact KDE above is <span class="math inline">\(\mathcal{O}(nm)\)</span> where <span class="math inline">\(n\)</span> is the number of sample points to estimate from and <span class="math inline">\(m\)</span> is the number of evaluation points (the points where you want to calculate the estimate).</p>
<p>There exist several methods to combat this complexity.</p>
</div>
<div id="simple-and-linear-binning" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Simple and linear binning</h2>
<p>The most straightforward way to decrease runtime is by limiting the number of sample points. This can be done by a binning routine, where the values at a smaller number of regular grid points are estimated from the original large number of sample points.
Given a set of sample points <span class="math inline">\(X = \{x_0, x_1, ..., x_k, ..., x_{n-1}, x_n\}\)</span> with weights <span class="math inline">\(w_k\)</span> and a set of equally spaced grid points <span class="math inline">\(G = \{g_0, g_1, ..., g_l, ..., g_{n-1}, g_M\}\)</span> where <span class="math inline">\(N &lt; n\)</span> we can assign an estimate (or a count) <span class="math inline">\(c_l\)</span> to each grid point <span class="math inline">\(g_l\)</span> and use the newly found <span class="math inline">\(g_l\)</span>’s to calculate the kernel density estimation instead.</p>
<p>This lowers the computational complexity down to <span class="math inline">\(\mathcal{O}(N \cdot m)\)</span>.</p>
<p>Depending on the number of grid points <span class="math inline">\(N\)</span> the estimate is either more accurate and slower or less accurate and faster. However as we will see in the comparison chapter later as well, even a grid of size <span class="math inline">\(1024\)</span> is enough to capture the true density with high accuracy given a million sample points<span class="citation"><sup><a href="references.html#ref-KDEpyDoc" role="doc-biblioref">7</a></sup></span>.</p>
<p>As described in the extensive overview by Artur Gramacki<span class="citation"><sup><a href="references.html#ref-gramacki2018fft" role="doc-biblioref">8</a></sup></span> simple binning or linear binning can be used, although the last is often preferred since it is more accurate and the difference in computational complexity is negligible.</p>
<p>Simple binning is just the standard process of taking a weighted histogram that is divided by the sum of the sample points weights (normalization). In one dimension simple binning is binary in that it assigns a sample point’s weight (<span class="math inline">\(w_k = 1\)</span> for an unweighted histogram) either to the grid point (bin) left or right of itself.</p>
<p><span class="math display" id="eq:simplebin">\[\begin{equation}
c_l = c(g_l) = \sum_{\substack{x_k \in X\\\frac{g_l + g_{l-1}}{2} &lt; x_k &lt; \frac{g_{l+1} + g_l}{2}}} w_k
\tag{2.2}
\end{equation}\]</span></p>
<p>Linear binning on the other hand assigns a fraction of the whole weight to both grid points (bins) on either side, proportional to the closeness of grid point and data point in relation to the distance between grid points (bin width).</p>
<p><span class="math display" id="eq:linbin">\[\begin{equation}
c_l = c(g_l) = \sum_{\substack{x_k \in X\\g_l &lt; x_k &lt; g_{l+1}}} \frac{g_{k+1}-x_k}{g_{l+1} - g_l} \cdot w_k + \sum_{\substack{x_k \in X\\g_{l-1} &lt; x_k &lt; g_l}} \frac{x_k - g_{l-1}}{g_{l+1} - g_l} \cdot w_k
\tag{2.3}
\end{equation}\]</span></p>
<p>The kernel density estimation can then be calculated as a mixture distribution of kernel functions located at the grid points, weighted with their associated grid count.</p>
<p><span class="math display" id="eq:kdebin">\[\begin{equation}
\widehat{f}_h(x) = \frac{1}{nh} \sum_{l=1}^N c_l \cdot K\Big(\frac{x-g_l}{h}\Big)
\tag{2.4}
\end{equation}\]</span></p>
</div>
<div id="using-convolution-and-the-fast-fourier-transform" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Using convolution and the Fast Fourier Transform</h2>
<p>Another technique to speed up the computation is rewriting the Kernel Density Estimation as convolution operation between the kernel function and the grid counts calculated by the binning routine given above.</p>
<p>By using the fact that a convolution is just a multiplication in Fourier space and only evaluating the KDE at grid points one can reduce the computational complexity down to <span class="math inline">\(\mathcal{O}(\log{N} \cdot N)\)</span>.<span class="citation"><sup><a href="references.html#ref-gramacki2018fft" role="doc-biblioref">8</a></sup></span></p>
<p>Using the equation <a href="mathematicalTheory.html#eq:kdebin">(2.4)</a> from above only evaluated at grid points gives us</p>
<p><span class="math display" id="eq:binkdegrid">\[\begin{equation}
\widehat{f}_h(g_j) = \frac{1}{nh} \sum_{l=1}^N c_l \cdot K\Big(\frac{g_j-g_l}{h}\Big) = \frac{1}{nh} \sum_{l=1}^N k_{j-l} \cdot c_l
\tag{2.5}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(k_{j-l} = K(\frac{g_j-g_l}{h})\)</span>.</p>
<p>If we set <span class="math inline">\(c_l = 0\)</span> for all <span class="math inline">\(l\)</span> not in the set <span class="math inline">\(\{1, ..., N\}\)</span> and notice that <span class="math inline">\(K(-x) = K(x)\)</span> we can extend equation <a href="mathematicalTheory.html#eq:binkdegrid">(2.5)</a> to a discrete convolution as follows</p>
<p><span class="math display" id="eq:binkdeconv">\[\begin{equation}
\widehat{f}_h(g_j) = \frac{1}{nh} \sum_{l=-N}^N k_{j-l} \cdot c_l = \vec{c} \ast \vec{k}
\tag{2.6}
\end{equation}\]</span></p>
<p>where the two vectors look like this</p>
<div class="figure"><span id="fig:ckFigure"></span>
<img src="figures/c_conv_k.png" alt="Vectors $\vec{c}$ and $\vec{k}$" width="723" />
<p class="caption">
Figure 2.1: Vectors <span class="math inline">\(\vec{c}\)</span> and <span class="math inline">\(\vec{k}\)</span>
</p>
</div>
<p>By using the well known convolution theorem we can fourier transform <span class="math inline">\(\vec{c}\)</span> and <span class="math inline">\(\vec{k}\)</span>, multiply them and inverse fourier transform them again to get the result of the discrete convolution.</p>
<p>However, due to the limitation of evaluating only at the grid points themselves, one needs to interpolate to get values for the estimated distribution at points in between.</p>
</div>
<div id="improved-sheather-jones-algorithm" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Improved Sheather-Jones Algorithm</h2>
<p>A different take on Kernel Density Estimators is described in the paper ‘Kernel density estimation by diffusion’ by Botev et al.<span class="citation"><sup><a href="references.html#ref-botev2010kernel" role="doc-biblioref">9</a></sup></span> The authors present a new adaptive kernel density estimator based on linear diffusion processes which also includes an estimation for the optimal bandwidth.</p>
<p>A more detailed and extensive explanation of the algorithm as well as an implementation in Matlab is given in the ‘Handbook of Monte Carlo Methods’<span class="citation"><sup><a href="references.html#ref-kroese2013handbook" role="doc-biblioref">10</a></sup></span> by the original paper authors. However the general idea is briefly sketched below.</p>
<p>The optimal bandwidth is often defined as the one that minimizes the mean integrated square error(<span class="math inline">\(MISE\)</span>)</p>
<p><span class="math display" id="eq:mise">\[\begin{equation}
MISE(h) = \mathbb{E}_f\int [\widehat{f}_{h,norm}(x,h) - f(x)]^2 dx
\tag{2.7}
\end{equation}\]</span></p>
<p>To find the optimal bandwidth it is useful to look at the second order derivative <span class="math inline">\(f^{(2)}\)</span> of the unknown distribution as it indicates how many peaks the distribution has and how steep they are. For a distribution with many narrow peaks close together a smaller bandwidth leads to better result since the peaks do not get smeared together to a single peak for instance.</p>
<p>As derived by Wand and Jones an asymptotically optimal bandwidth <span class="math inline">\(h_{AMISE}\)</span> which minimizes a first-order asymptotic appoximation of the <span class="math inline">\(MISE\)</span> of the bandwidth is then given by<span class="citation"><sup><a href="references.html#ref-wand1994kernel" role="doc-biblioref">11</a></sup></span></p>
<p><span class="math display" id="eq:hamise">\[\begin{equation}
h_{AMISE}(x,h) = \Big( \frac{1}{2N\sqrt{\pi} \| f^{(2)}(x,h)\|^2}\Big)^{\frac{1}{5}}
\tag{2.8}
\end{equation}\]</span></p>
<p>As Sheather and Jones showed, this second order derivative can be estimated, starting from an even higher order derivative <span class="math inline">\(\|f^{(l+2)}\|^2\)</span> by using the fact that <span class="math inline">\(\|f^{(j)}\|^2 = (-1)^j \mathbb{E}_f[f^{(2j)}(X)], \text{ } j\geq 1\)</span></p>
<p><span class="math display" id="eq:hj">\[\begin{equation}
h_j=\left(\frac{1+1 / 2^{j+1 / 2}}{3} \frac{1 \times 3 \times 5 \times \cdots \times(2 j-1)}{N \sqrt{\pi / 2}\left\|f^{(j+1)}\right\|^{2}}\right)^{1 /(3+2 j)} = \gamma_j(h_{j+1})
\tag{2.9}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(h_j\)</span> is the optimal bandwidth for the <span class="math inline">\(j\)</span>-th derivative of <span class="math inline">\(f\)</span>.</p>
<p>Their proposed plug-in method works as follows:</p>
<ol style="list-style-type: decimal">
<li>Compute <span class="math inline">\(\|\widehat{f}^{(l+2)}\|^2\)</span> by assuming that f is the normal pdf with mean and variance estimated from the sample data</li>
<li>Using <span class="math inline">\(\|\widehat{f}^{(l)}\|^2\)</span> compute <span class="math inline">\(h_{l+1}\)</span></li>
<li>Using <span class="math inline">\(h_{l+1}\)</span> compute <span class="math inline">\(\|\widehat{f}^{(l+1)}\|^2\)</span></li>
<li>Repeat steps 2 and 3 to compute <span class="math inline">\(h^{l-1}\)</span> and so on until
<span class="math inline">\(\|\widehat{f}^{(2)}\|^2\)</span> is calculated</li>
<li>Use <span class="math inline">\(\|\widehat{f}^{(2)}\|^2\)</span> to compute <span class="math inline">\(h_{AMISE}\)</span></li>
</ol>
<p>The weakest point of this procedure is the assumption that the true distribution is a Gaussian density function in order to compute <span class="math inline">\(\|\widehat{f}^{(l+2)}\|^2\)</span>. This can lead to arbitrarily bad estimates of <span class="math inline">\(h_{AMISE}\)</span>, when the true distribution is far from being normal.</p>
<p>Therefore Botev et al took this idea further<span class="citation"><sup><a href="references.html#ref-botev2010kernel" role="doc-biblioref">9</a></sup></span>. Given the function <span class="math inline">\(\gamma^{[k]}\)</span> such that</p>
<p><span class="math display" id="eq:gamma">\[\begin{equation}
\gamma^{[h]}(h)=\underbrace{\gamma_{1}\left(\cdots \gamma_{h-1}\left(\gamma_{h}\right.\right.}_{k \text { times }}(h)) \cdots)
\tag{2.10}
\end{equation}\]</span></p>
<p><span class="math inline">\(h_{AMISE}\)</span> can be calculated as</p>
<p><span class="math display" id="eq:hamisegamma">\[\begin{equation}
\begin{array}{l}
h_{AMISE} =\xi h_{1}=\xi \gamma^{[1]}\left(h_{2}\right)=\xi \gamma^{[2]}\left(h_{3}\right)=\cdots=\xi \gamma^{[l]}\left(h_{l+1}\right) \\
\xi=\left(\frac{6 \sqrt{2}-3}{7}\right)^{2 / 5} \approx 0.90
\end{array}
\tag{2.11}
\end{equation}\]</span></p>
<p>By setting <span class="math inline">\(h_{AMISE}=h_{l+1}\)</span> and using fixed point iteration to solve the equation</p>
<p><span class="math display" id="eq:hamisegamma2">\[\begin{equation}
h_{AMISE} = \xi \gamma^{[l]}(h_{AMISE})
\tag{2.12}
\end{equation}\]</span></p>
<p>the optimal bandwidth <span class="math inline">\(h_{AMISE}\)</span> can be found directly.</p>
<p>This eliminates the need to assume normally distributed data for the initial estimate and leads to improved performance, especially for density distributions that are far from normal as seen in the next chapter.</p>
<p>According to their paper increasing <span class="math inline">\(l\)</span> beyond <span class="math inline">\(l=5\)</span> does not increase the accuracy in any practically meaningful way.
The computation is especially efficient if <span class="math inline">\(\gamma^{[5]}\)</span> is computed using the Discrete Cosine Transform - an FFT related transformation.</p>
<p>The optimal bandwidth <span class="math inline">\(h_{AMISE}\)</span> can then either be used for other kernel density estimation methods (like the FFT-approach discussed above) or also to compute the kernel density estimation directly using another Discrete Cosine Transform.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="currentState.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/AstroViking/ba-thesis/edit/master/chapters/02-theory.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/AstroViking/ba-thesis/blob/master/chapters/02-theory.Rmd",
"text": null
},
"download": ["thesis.pdf", "thesis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
