# Theory {#mathematicalTheory}

## Exact Kernel Density Estimation

An exact Kernel Density Estimation can be calculated as

\begin{equation}
\widehat{f}_h(x) = \frac{1}{nh} \sum_{k=1}^n K\Big(\frac{x-x_k}{h}\Big)
(\#eq:kde)
\end{equation}

where $K(x)$ is called the kernel function, which defines the range and size of influence of a single data point over the estimation. Most typically a simple Gaussian distribution is used as kernel function.
The wider the kernel function is, the farther  is the influence of a single data point, this is characterized by the bandwidth paramter $h$. 

The computational complexity of the exact KDE above is $\mathcal{O}(nm)$ where $n$ is the number of sample points to estimate from and $m$ is the number of evaluation points (the points where you want to calculate the estimate).

There exist several methods to combat this complexity. 

## Simple and linear binning

The most straightforward way to decrease runtime is by limiting the number of sample points. This can be done by a binning routine, where the values at a smaller number of regular grid points are estimated from the original large number of sample points.
Given a set of sample points $X = \{x_0, x_1, ..., x_k, ..., x_{n-1}, x_n\}$ with weights $w_k$ and a set of equally spaced grid points $G = \{g_0, g_1, ..., g_l, ..., g_{n-1}, g_M\}$ where $N < n$ we can assign an estimate (or a count) $c_l$ to each grid point $g_l$ and use the newly found $g_l$'s to calculate the kernel density estimation instead. 

This lowers the computational complexity down to $\mathcal{O}(N \cdot m)$. 

Depending on the number of grid points $N$ the estimate is either more accurate and slower or less accurate and faster. However as we will see in the comparison chapter later as well, even a grid of size $1024$ is enough to capture the true density with high accuracy given a million sample points[@KDEpyDoc].

As described in the extensive overview by Artur Gramacki[@gramacki2018fft] simple binning or linear binning can be used, although the last is often preferred since it is more accurate and the difference in computational complexity is negligible.

Simple binning is just the standard process of taking a weighted histogram that is divided by the sum of the sample points weights (normalization). In one dimension simple binning is binary in that it assigns a sample point's weight ($w_k = 1$ for an unweighted histogram) either to the grid point (bin) left or right of itself.

\begin{equation}
c_l = c(g_l) = \sum_{\substack{x_k \in X\\\frac{g_l + g_{l-1}}{2} < x_k < \frac{g_{l+1} + g_l}{2}}} w_k
(\#eq:simplebin)
\end{equation}

Linear binning on the other hand assigns a fraction of the whole weight to both grid points (bins) on either side, proportional to the closeness of grid point and data point in relation to the distance between grid points (bin width).

\begin{equation}
c_l = c(g_l) = \sum_{\substack{x_k \in X\\g_l < x_k < g_{l+1}}} \frac{g_{k+1}-x_k}{g_{l+1} - g_l} \cdot w_k + \sum_{\substack{x_k \in X\\g_{l-1} < x_k < g_l}} \frac{x_k - g_{l-1}}{g_{l+1} - g_l} \cdot w_k
(\#eq:linbin)
\end{equation}

The kernel density estimation can then be calculated as a mixture distribution of kernel functions located at the grid points, weighted with their associated grid count.

\begin{equation}
\widehat{f}_h(x) = \frac{1}{nh} \sum_{l=1}^N c_l \cdot K\Big(\frac{x-g_l}{h}\Big)
(\#eq:kdebin)
\end{equation}


## Using convolution and the Fast Fourier Transform

Another technique to speed up the computation is rewriting the Kernel Density Estimation as convolution operation between the kernel function and the grid counts calculated by the binning routine given above. 

By using the fact that a convolution is just a multiplication in Fourier space and only evaluating the KDE at grid points one can reduce the computational complexity down to $\mathcal{O}(\log{N} \cdot N)$.[@gramacki2018fft]

Using the equation \@ref(eq:kdebin) from above only evaluated at grid points gives us

\begin{equation}
\widehat{f}_h(g_j) = \frac{1}{nh} \sum_{l=1}^N c_l \cdot K\Big(\frac{g_j-g_l}{h}\Big) = \frac{1}{nh} \sum_{l=1}^N k_{j-l} \cdot c_l
(\#eq:binkdegrid)
\end{equation}

where $k_{j-l} = K(\frac{g_j-g_l}{h})$.

If we set $c_l = 0$ for all $l$ not in the set $\{1, ..., N\}$ and notice that $K(-x) = K(x)$ we can extend equation \@ref(eq:binkdegrid) to a discrete convolution as follows

\begin{equation}
\widehat{f}_h(g_j) = \frac{1}{nh} \sum_{l=-N}^N k_{j-l} \cdot c_l = \vec{c} \ast \vec{k}
(\#eq:binkdeconv)
\end{equation}

where the two vectors look like this

```{r, ckFigure, fig.cap="Vectors $\\vec{c}$ and $\\vec{k}$", echo=FALSE}
knitr::include_graphics('figures/c_conv_k.png')
```

By using the well known convolution theorem we can fourier transform $\vec{c}$ and $\vec{k}$, multiply them and inverse fourier transform them again to get the result of the discrete convolution.

However, due to the limitation of evaluating only at the grid points themselves, one needs to interpolate to get values for the estimated distribution at points in between.


## Improved Sheather-Jones Algorithm

A different take on Kernel Density Estimators is described in the paper 'Kernel density estimation by diffusion' by Botev et al.[@botev2010kernel]. The authors present a new adaptive kernel density estimator based on linear diffusion processes which also includes an estimation for the optimal bandwidth.

The algorithm is quite difficult to understand, a detailed explanation is given in the 'Handbook of Monte Carlo Methods'[@kroese2013handbook] by the original paper authors. However the general idea is briefly sketched below.

A critical insight is that the Gaussian kernel density estimator $\widehat{f}_{h,norm}$ is the solution of the partial differential equation

\begin{equation}
\frac{\partial}{\partial t} \widehat{f}_{h,norm}(x,t) = \frac{1}
{2} \frac{\partial^2}{\partial x^2} \widehat{f}_{h,norm}(x,t),\text{ } t>0 
(\#eq:heatpde)
\end{equation}

with $x \in \mathbb{R}$, $\lim_{x\rightarrow \pm \infty}{\widehat{f}_{h,norm}(x,t) = 0}$ and initial condition $\widehat{f}_{h,norm}(x,0) = \Delta(x)$, where $\Delta(x) = \frac{1}{N} \sum_{k=0}^N \delta_{x_k}(x)$ is the empirical density of the given sample points $X = \{x_0, x_1, ..., x_k, ..., x_{n-1}, x_n\}$ and $\delta_{x_k}(x)$ is the Dirac measure at $x_k$. 

This means the kernel density estimator can be obtained by evolving the solution of the partial differential equation \@ref(eq:heatpde) up to time $t$. Another key observation is that \@ref(eq:heatpde) can be solved on a finite domain efficiently using the fast cosine transform - an FFT-related transform.[@kroese2013handbook]

The optimal bandwidth is often defined as the one that minimizes the mean integrated square error($MISE$)

\begin{equation}
MISE(t) = \mathbb{E}_f\int [\widehat{f}_{h,norm}(x,t) - f(x)]^2 dx
(\#eq:mise)
\end{equation}

An asymptotically optimal value $t^{\ast}$ which minimizes a first-order asymptotic appoximation of the $MISE$ is then given by[@kroese2013handbook]

\begin{equation}
t^{\ast} = \Big( \frac{1}{2N\sqrt{\pi} \| f''\|^2}\Big)^{\frac{2}{5}}
(\#eq:tstar)
\end{equation}

Using the fact that $\|f^{(j)}\|^2 = (-1)^j \mathbb{E}_f[f^{(2j)}(X)], \text{ } j\geq 1$ and an initial estimation for $\|\widehat{f}_{h,norm}^{(l+2)}\|^2$  for some $l \geq 3$ one can then iteratively get an estimation for $\|\widehat{f}_{h,norm}^{(2)}\|^2$ which can in turn be used to estimate $t^{\ast}$ instead of $\|f''\|^2$. According to their handbook $l = 7$ is a suitable value to yield good practical results.

One can evolve the solution of the partial differential equation \@ref(eq:heatpde) up to time $t^{\ast}$ to get the kernel density estimation or simply use $t^{\ast}$ to compute the optimal bandwidth according to the $MISE$ and use it for other kernel density estimation methods like the FFT-approach discussed above.

The improvment compared to the standard Sheather-Jones plug-on method[@sheather1991reliable] consists in the fact that the initial estimation of $\|\widehat{f}_{h,norm}^{(l+2)}\|^2$ is calculated by solving the partial differental equation using the fast cosine transform as described above, eliminating the need to assume normally distributed data for the initial estimate and leading to improved performance, especially for density distributions that are far from normal as seen in the next chapter.

One shortcoming of the improved Sheather-Jones algorithm (ISJ) is that with few data points to estimate from, it is not guarenteed to converge. If that happens, one has to use the exact, binned or FFT kernel density estimators as described above.