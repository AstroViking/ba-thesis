# Implementation {#tensorflowImplementation}

## Exact Kernel Density Estimation

The implementation of an exact Kernel Density Estimation in TensorFlow is straightforward. As described in the original Tensorflow Probability Paper[@googleTFP], a KDE can be constructed by using its MixtureSameFamily Distribution, given sampled `data` as follows

```{python, eval=FALSE}

from tensorflow_probability import distributions as tfd

f = lambda x: tfd.Independent(tfd.Normal(loc=x, scale=1.))
n = data.shape[0].value

kde = tfd.MixtureSameFamily(
    mixture_distribution=tfd.Categorical(
        probs=[1 / n] * n),
    components_distribution=f(data))
```

Interestingly, due to the smart encapsulated structure of TensorFlow Probability we can use any distribution of the loc-scale family type as a kernel as long as it follows the Distribution contract in TensorFlow Probability. If the used Kernel has only bounded support, the implementation proposed in this paper allows to specify the support upon instantiation of the class. If the Kernel has infinite support (like a Gaussian kernel for instance) a practical support estimate is calculated by searching for approximate roots with Brent's method[@brent1971algorithm] implemented for TensorFlow in the python package `tf_quant_finance` by Google. This allows us to speed up the calculation.

However calculating an exact kernel density estimation is not always feasible as this can take a long time with a huge collection of data points, especially in high energy physics. By implementing it in TensorFlow we already get a significant speed up compared to implementations in native Python, since most of TensorFlow is actually implemented in C++ and the code is optimized before running. Even if the theoretical computational complexity remains the same.


## Simple and linear Binning

Simple binning is already implemented in TensorFlow in the function `tf.histogram_fixed_width`.

Implementing linear binning efficiently with TensorFlow is a bit tricky since loops should be avoided. However with some inspiration from the KDEpy package[@KDEpy] this can be done without using loops at all. 

First, every data point $x_k$  can be described by an integral part $x^{integral}_k$ (equal to its nearest left grid point number $l$ = $x^{integral}_k$) plus some fractional part $x^{fractional}_k$ (corresponding to the additional distance between grid point $g_l$ and data point $x_k$).

Then we can solve the linear binning in the following way.

For data points on the right side of the grid point $g_l$: The fractional parts of the data points are summed if the integral parts equal $l$. 

For data points on the left side of the grid point $g_l$: $1$ minus the fractional parts of the data points are summed if the integral parts equal $l-1$.

Including the weights this looks as follows

\begin{equation}
c_l = c(g_l) = \sum_{\substack{x^{fractional}_k  \in X^{fractional}\\l = x^{integral}_k}} x^{fractional}_k \cdot w_k + \sum_{\substack{x^{fractional}_k  \in X^{fractional}\\l = x^{integral}_k + 1}} (1-x^{fractional}_k) \cdot w_k
(\#eq:linbinnoloop)
\end{equation}


Left and right side sums can then be calculated efficiently with the TensorFlow function `tf.math.bincount`.


## Using convolution and the Fast Fourier Transform

In TensorFlow one-dimensional convolutions are efficiently implemented already if we use `tf.nn.conv1d`. In benchmarking using this method proved significantly faster than using `tf.signal.rfft` and `tf.signal.irfft` to transform, multiply and inverse transform the vectors, which is implemented as an alternative as well.

This algorithm is implemented as its own class since it does not represent a complete mixture distribution anymore but calculates just the density distribution values at the specified grid points. To still infer values for other points in the range of $x$ `tfp.math.interp_regular_1d_grid` is used, which computes a linear interpolation of values between the grid.

## Improved Sheather Jones Algorithm

To find the roots for equation \@ref(eq:hamisegamma2) Brent's method[@brent1971algorithm] was used, which is implemented in TensorFlow in the python package `tf_quant_finance`. For the Discrete Cosine Transform `tf.signal.dct` is used.

## Source Code

The source code of the newly proposed implementation can be found at [https://github.com/AstroViking/tf-kde].