# Comparison {#comparison}

```{r, setup, echo=FALSE, eval=TRUE}
library('reticulate')
use_condaenv('ba-thesis')
```

```{python, pythonSetup, echo = FALSE, eval=TRUE}
from tf_kde.benchmark import runner
from matplotlib import pyplot as plt
import pandas as pd

distributions_to_evaluate = [
    'Gaussian',
    'Uniform',
    'Bimodal',
    'SkewedBimodal',
    'Claw',
    'AsymmetricDoubleClaw'
]

xlim = [-8, 8]

unrestricted_runtimes = pd.read_pickle('./benchmark/cpu/unrestricted_runtimes.pkl')
restricted_runtimes = pd.read_pickle('./benchmark/cpu/restricted_runtimes.pkl')

unrestricted_estimations = pd.read_pickle('./benchmark/cpu/unrestricted_estimations.pkl')
restricted_estimations = pd.read_pickle('./benchmark/cpu/restricted_estimations.pkl')

```

To show the efficiency and performance of the Kernel Density Estimation methods implemented with TensorFlow a benchmarking suite was developed. It consists of three parts, a collection of distributions, a collection of methods to compare and a runner module that implements helper methods to execute the methods to test against the different distributions and plot the generated dataset nicely.

## Benchmark setup

To compare the different implementations multiple popular test distributions mentioned in Wand et al.[@wand1994kernel] were used. A simple normal distribution, a simple uniform distribution, a bimodal distribution comprised of two normals, a skewed bimodal distribution, a claw distribution that has spikes and one called asymmetric double claw that has different sized spikes left and right. All comparisons were made using a standard Gaussian Kernel. Although all loc-scale family distributions of TensorFlow Probability may be used for the new implmentation proposed in this paper may be used, the Gaussian kernel is the most used one and provides best reference to compare different implementations against eachother.

```{r, distDisplay, echo = FALSE, fig.cap="Distribution used to benchmark"}
knitr::include_graphics('./benchmark/cpu/distributions.pdf')
```
## Basic implementation against Binned and FFT implementations

```{python}
figure, axes = runner.plot_estimations(restricted_estimations, distributions_to_evaluate, 1e3, ['ZfitExact', 'ZfitExactwithAdaptiveBandwidth', 'ZfitBinned'])
figure
```

!TODO: 

### Runtime

```{r, compareRuntimesBasic, echo = FALSE, fig.cap=""}
knitr::include_graphics('./benchmark/cpu/distributions.pdf')
```

```{r, compareRuntimeTable, echo=FALSE, eval=TRUE}
#knitr::kable(py$restricted_runtimes, booktabs = TRUE, caption = 'Comparing additional methods')
```


### Accuracy

```{python, compareAccuracyBasic, echo = FALSE, fig.cap=""}
#knitr::include_graphics('../plots/compare...')
```


## New implementation against KDEpy

### Runtime

```{python, compareRuntimesKDEpy, echo = FALSE, fig.cap=""}
#knitr::include_graphics('../plots/compare...')
```


```{r, compareRuntimesKDEpyTable, echo=FALSE, eval=TRUE}
#knitr::kable(py$runtimes, booktabs = TRUE, caption = 'Runtime comparison')
```

### Accuracy

```{python, compareAccuracyKDEpy, echo = FALSE, fig.cap=""}
#knitr::include_graphics('../plots/compare...')
```

## New implementation run with GPU support

### Runtime

```{python, compareRuntimesGPU, echo = FALSE, fig.cap=""}
#knitr::include_graphics('../plots/compare...')
```


```{r, compareRuntimesGPUTable, echo=FALSE, eval=TRUE}
#knitr::kable(py$runtimes, booktabs = TRUE, caption = 'Runtime comparison')
```

### Accuracy

```{python, compareAccuracyGPU, echo = FALSE, fig.cap=""}
#knitr::include_graphics('../plots/compare...')
```
